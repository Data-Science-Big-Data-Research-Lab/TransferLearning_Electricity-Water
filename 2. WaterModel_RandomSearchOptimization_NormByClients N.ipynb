{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.metrics\n",
    "from keras import initializers\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def read_csv(file):\n",
    "    dataframe = pd.read_csv(file=file)\n",
    "    return dataframe\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "\n",
    "    model.add(keras.layers.Dense(12, activation=\"tanh\"))\n",
    "\n",
    "    for i in range(hp.Int(\"num_layers\", 5, 15)):\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                units=hp.Int(\"units_\" + str(i), min_value=20, max_value=80, step=10),\n",
    "                activation=\"tanh\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    model.add(keras.layers.Dense(1, activation=\"tanh\"))\n",
    "\n",
    "    optimizer = Adam(\n",
    "    learning_rate=hp.Float('learning_rate', min_value=0.00001, max_value=0.001, sampling='log')\n",
    "    )\n",
    "\n",
    "    model.compile(loss=\"mean_absolute_error\",\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=[\"mean_absolute_error\",\"mean_squared_error\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con punto y coma como delimitador\n",
    "data = pd.read_csv('Cluster0ReadyToNN.csv', delimiter=';')\n",
    "\n",
    "# Crear un diccionario para almacenar los objetos scaler por grupo\n",
    "scalers = {}\n",
    "\n",
    "# Iterar sobre los grupos únicos en Column15\n",
    "for group in data['Column15'].unique():\n",
    "    # Filtrar datos por grupo\n",
    "    group_data = data[data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas para normalización (las 13 primeras)\n",
    "    features = group_data.iloc[:, :13]\n",
    "\n",
    "    # Normalizar los datos con MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(features)\n",
    "\n",
    "    # Almacenar el scaler en el diccionario\n",
    "    scalers[group] = scaler\n",
    "\n",
    "    # Actualizar el DataFrame con los datos normalizados\n",
    "    data.loc[data['Column15'] == group, 'Column1':'Column13'] = normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento (70%) y temporal (30%)\n",
    "train_temp_data, test_data = train_test_split(data, test_size=0.3, stratify=data['Column15'], random_state=0)\n",
    "\n",
    "# Dividir el temporal en entrenamiento (70%) y validación (30%)\n",
    "train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, stratify=train_temp_data['Column15'], random_state=0)\n",
    "\n",
    "# Separar características (X) y columna objetivo (y) para entrenamiento\n",
    "X_train_norm = train_data.iloc[:, :12]\n",
    "y_train_norm = train_data['Column13']\n",
    "\n",
    "# Separar características (X) y columna objetivo (y) para validación\n",
    "X_val_norm = validation_data.iloc[:, :12]\n",
    "y_val_norm = validation_data['Column13']\n",
    "\n",
    "# Separar características (X) y columna objetivo (y) para prueba\n",
    "X_test_norm = test_data.iloc[:, :12]\n",
    "y_test_norm = test_data['Column13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "epochs_val = 100\n",
    "batch_size_val = 256\n",
    "max_trials_val = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 Complete [00h 02m 03s]\n",
      "val_loss: 0.08507491648197174\n",
      "\n",
      "Best val_loss So Far: 0.0842648521065712\n",
      "Total elapsed time: 00h 33m 28s\n",
      "Results summary\n",
      "Results in my_tuner_dir\\DFFNN_optimization_norm\n",
      "Showing 10 best trials\n",
      "Objective(name=\"val_loss\", direction=\"min\")\n",
      "\n",
      "Trial 09 summary\n",
      "Hyperparameters:\n",
      "num_layers: 13\n",
      "units_0: 20\n",
      "units_1: 50\n",
      "units_2: 40\n",
      "units_3: 70\n",
      "units_4: 20\n",
      "learning_rate: 0.0009226604447116868\n",
      "units_5: 20\n",
      "units_6: 50\n",
      "units_7: 20\n",
      "units_8: 20\n",
      "units_9: 60\n",
      "units_10: 40\n",
      "units_11: 80\n",
      "units_12: 30\n",
      "units_13: 30\n",
      "units_14: 60\n",
      "Score: 0.0842648521065712\n",
      "\n",
      "Trial 22 summary\n",
      "Hyperparameters:\n",
      "num_layers: 8\n",
      "units_0: 80\n",
      "units_1: 80\n",
      "units_2: 20\n",
      "units_3: 30\n",
      "units_4: 40\n",
      "learning_rate: 0.0006857926194454209\n",
      "units_5: 40\n",
      "units_6: 70\n",
      "units_7: 40\n",
      "units_8: 20\n",
      "units_9: 60\n",
      "units_10: 80\n",
      "units_11: 30\n",
      "units_12: 40\n",
      "units_13: 50\n",
      "units_14: 70\n",
      "Score: 0.08449201285839081\n",
      "\n",
      "Trial 12 summary\n",
      "Hyperparameters:\n",
      "num_layers: 6\n",
      "units_0: 20\n",
      "units_1: 50\n",
      "units_2: 50\n",
      "units_3: 50\n",
      "units_4: 70\n",
      "learning_rate: 0.0002870495391129379\n",
      "units_5: 60\n",
      "units_6: 20\n",
      "units_7: 70\n",
      "units_8: 60\n",
      "units_9: 20\n",
      "units_10: 30\n",
      "units_11: 60\n",
      "units_12: 60\n",
      "units_13: 60\n",
      "units_14: 30\n",
      "Score: 0.08461632579565048\n",
      "\n",
      "Trial 03 summary\n",
      "Hyperparameters:\n",
      "num_layers: 10\n",
      "units_0: 50\n",
      "units_1: 50\n",
      "units_2: 40\n",
      "units_3: 70\n",
      "units_4: 80\n",
      "learning_rate: 0.00029073893028160313\n",
      "units_5: 50\n",
      "units_6: 60\n",
      "units_7: 40\n",
      "units_8: 80\n",
      "units_9: 70\n",
      "units_10: 30\n",
      "units_11: 40\n",
      "Score: 0.08475863188505173\n",
      "\n",
      "Trial 17 summary\n",
      "Hyperparameters:\n",
      "num_layers: 13\n",
      "units_0: 40\n",
      "units_1: 50\n",
      "units_2: 50\n",
      "units_3: 40\n",
      "units_4: 60\n",
      "learning_rate: 0.00014314667651273776\n",
      "units_5: 30\n",
      "units_6: 50\n",
      "units_7: 20\n",
      "units_8: 50\n",
      "units_9: 60\n",
      "units_10: 40\n",
      "units_11: 70\n",
      "units_12: 70\n",
      "units_13: 80\n",
      "units_14: 20\n",
      "Score: 0.08484265208244324\n",
      "\n",
      "Trial 16 summary\n",
      "Hyperparameters:\n",
      "num_layers: 12\n",
      "units_0: 70\n",
      "units_1: 70\n",
      "units_2: 70\n",
      "units_3: 30\n",
      "units_4: 50\n",
      "learning_rate: 0.00020312059283255943\n",
      "units_5: 20\n",
      "units_6: 80\n",
      "units_7: 40\n",
      "units_8: 80\n",
      "units_9: 70\n",
      "units_10: 60\n",
      "units_11: 80\n",
      "units_12: 60\n",
      "units_13: 80\n",
      "units_14: 20\n",
      "Score: 0.0848769024014473\n",
      "\n",
      "Trial 13 summary\n",
      "Hyperparameters:\n",
      "num_layers: 8\n",
      "units_0: 30\n",
      "units_1: 30\n",
      "units_2: 20\n",
      "units_3: 50\n",
      "units_4: 20\n",
      "learning_rate: 0.0005001588003685089\n",
      "units_5: 70\n",
      "units_6: 40\n",
      "units_7: 40\n",
      "units_8: 20\n",
      "units_9: 50\n",
      "units_10: 40\n",
      "units_11: 40\n",
      "units_12: 30\n",
      "units_13: 20\n",
      "units_14: 50\n",
      "Score: 0.08503054082393646\n",
      "\n",
      "Trial 24 summary\n",
      "Hyperparameters:\n",
      "num_layers: 14\n",
      "units_0: 40\n",
      "units_1: 80\n",
      "units_2: 20\n",
      "units_3: 70\n",
      "units_4: 50\n",
      "learning_rate: 5.788602293070543e-05\n",
      "units_5: 20\n",
      "units_6: 70\n",
      "units_7: 30\n",
      "units_8: 60\n",
      "units_9: 60\n",
      "units_10: 40\n",
      "units_11: 80\n",
      "units_12: 60\n",
      "units_13: 20\n",
      "units_14: 80\n",
      "Score: 0.08507491648197174\n",
      "\n",
      "Trial 07 summary\n",
      "Hyperparameters:\n",
      "num_layers: 15\n",
      "units_0: 60\n",
      "units_1: 20\n",
      "units_2: 30\n",
      "units_3: 30\n",
      "units_4: 80\n",
      "learning_rate: 0.0002768506310740926\n",
      "units_5: 50\n",
      "units_6: 60\n",
      "units_7: 20\n",
      "units_8: 40\n",
      "units_9: 20\n",
      "units_10: 20\n",
      "units_11: 30\n",
      "units_12: 20\n",
      "units_13: 20\n",
      "units_14: 20\n",
      "Score: 0.08510857820510864\n",
      "\n",
      "Trial 15 summary\n",
      "Hyperparameters:\n",
      "num_layers: 13\n",
      "units_0: 80\n",
      "units_1: 40\n",
      "units_2: 60\n",
      "units_3: 60\n",
      "units_4: 80\n",
      "learning_rate: 0.0008693481536242379\n",
      "units_5: 30\n",
      "units_6: 30\n",
      "units_7: 70\n",
      "units_8: 80\n",
      "units_9: 50\n",
      "units_10: 80\n",
      "units_11: 50\n",
      "units_12: 60\n",
      "units_13: 70\n",
      "units_14: 80\n",
      "Score: 0.08512835204601288\n",
      "Best Model:  <keras.src.engine.sequential.Sequential object at 0x0000025786E5A6D0>\n",
      "Epoch 1/100\n",
      "327/327 [==============================] - 4s 7ms/step - loss: 0.0847 - mean_absolute_error: 0.0847 - mean_squared_error: 0.0259 - val_loss: 0.0862 - val_mean_absolute_error: 0.0862 - val_mean_squared_error: 0.0273\n",
      "Epoch 2/100\n",
      "327/327 [==============================] - 2s 6ms/step - loss: 0.0847 - mean_absolute_error: 0.0847 - mean_squared_error: 0.0259 - val_loss: 0.0868 - val_mean_absolute_error: 0.0868 - val_mean_squared_error: 0.0259\n",
      "Epoch 3/100\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0848 - mean_absolute_error: 0.0848 - mean_squared_error: 0.0259 - val_loss: 0.0847 - val_mean_absolute_error: 0.0847 - val_mean_squared_error: 0.0262\n",
      "Epoch 4/100\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0846 - mean_absolute_error: 0.0846 - mean_squared_error: 0.0259 - val_loss: 0.0849 - val_mean_absolute_error: 0.0849 - val_mean_squared_error: 0.0263\n",
      "Epoch 5/100\n",
      "327/327 [==============================] - 2s 6ms/step - loss: 0.0846 - mean_absolute_error: 0.0846 - mean_squared_error: 0.0259 - val_loss: 0.0855 - val_mean_absolute_error: 0.0855 - val_mean_squared_error: 0.0267\n",
      "Epoch 6/100\n",
      "327/327 [==============================] - 2s 6ms/step - loss: 0.0845 - mean_absolute_error: 0.0845 - mean_squared_error: 0.0258 - val_loss: 0.0846 - val_mean_absolute_error: 0.0846 - val_mean_squared_error: 0.0263\n",
      "Epoch 7/100\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0845 - mean_absolute_error: 0.0845 - mean_squared_error: 0.0258 - val_loss: 0.0858 - val_mean_absolute_error: 0.0858 - val_mean_squared_error: 0.0260\n",
      "Epoch 8/100\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0846 - mean_absolute_error: 0.0846 - mean_squared_error: 0.0258 - val_loss: 0.0851 - val_mean_absolute_error: 0.0851 - val_mean_squared_error: 0.0263\n",
      "Epoch 9/100\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0844 - mean_absolute_error: 0.0844 - mean_squared_error: 0.0258 - val_loss: 0.0845 - val_mean_absolute_error: 0.0845 - val_mean_squared_error: 0.0258\n",
      "Epoch 10/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0844 - mean_absolute_error: 0.0844 - mean_squared_error: 0.0258 - val_loss: 0.0850 - val_mean_absolute_error: 0.0850 - val_mean_squared_error: 0.0258\n",
      "Epoch 11/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0843 - mean_absolute_error: 0.0843 - mean_squared_error: 0.0257 - val_loss: 0.0852 - val_mean_absolute_error: 0.0852 - val_mean_squared_error: 0.0258\n",
      "Epoch 12/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0844 - mean_absolute_error: 0.0844 - mean_squared_error: 0.0258 - val_loss: 0.0850 - val_mean_absolute_error: 0.0850 - val_mean_squared_error: 0.0267\n",
      "Epoch 13/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0842 - mean_absolute_error: 0.0842 - mean_squared_error: 0.0257 - val_loss: 0.0844 - val_mean_absolute_error: 0.0844 - val_mean_squared_error: 0.0259\n",
      "Epoch 14/100\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0842 - mean_absolute_error: 0.0842 - mean_squared_error: 0.0257 - val_loss: 0.0847 - val_mean_absolute_error: 0.0847 - val_mean_squared_error: 0.0263\n",
      "Epoch 15/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0842 - mean_absolute_error: 0.0842 - mean_squared_error: 0.0257 - val_loss: 0.0842 - val_mean_absolute_error: 0.0842 - val_mean_squared_error: 0.0262\n",
      "Epoch 16/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0841 - mean_absolute_error: 0.0841 - mean_squared_error: 0.0257 - val_loss: 0.0848 - val_mean_absolute_error: 0.0848 - val_mean_squared_error: 0.0265\n",
      "Epoch 17/100\n",
      "327/327 [==============================] - 1s 3ms/step - loss: 0.0843 - mean_absolute_error: 0.0843 - mean_squared_error: 0.0257 - val_loss: 0.0858 - val_mean_absolute_error: 0.0858 - val_mean_squared_error: 0.0265\n",
      "Epoch 18/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0842 - mean_absolute_error: 0.0842 - mean_squared_error: 0.0257 - val_loss: 0.0849 - val_mean_absolute_error: 0.0849 - val_mean_squared_error: 0.0262\n",
      "Epoch 19/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0840 - mean_absolute_error: 0.0840 - mean_squared_error: 0.0256 - val_loss: 0.0847 - val_mean_absolute_error: 0.0847 - val_mean_squared_error: 0.0267\n",
      "Epoch 20/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0840 - mean_absolute_error: 0.0840 - mean_squared_error: 0.0257 - val_loss: 0.0860 - val_mean_absolute_error: 0.0860 - val_mean_squared_error: 0.0265\n",
      "Epoch 21/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0839 - mean_absolute_error: 0.0839 - mean_squared_error: 0.0256 - val_loss: 0.0863 - val_mean_absolute_error: 0.0863 - val_mean_squared_error: 0.0254\n",
      "Epoch 22/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0841 - mean_absolute_error: 0.0841 - mean_squared_error: 0.0256 - val_loss: 0.0853 - val_mean_absolute_error: 0.0853 - val_mean_squared_error: 0.0270\n",
      "Epoch 23/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0840 - mean_absolute_error: 0.0840 - mean_squared_error: 0.0256 - val_loss: 0.0853 - val_mean_absolute_error: 0.0853 - val_mean_squared_error: 0.0269\n",
      "Epoch 24/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0840 - mean_absolute_error: 0.0840 - mean_squared_error: 0.0256 - val_loss: 0.0845 - val_mean_absolute_error: 0.0845 - val_mean_squared_error: 0.0259\n",
      "Epoch 25/100\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0840 - mean_absolute_error: 0.0840 - mean_squared_error: 0.0256 - val_loss: 0.0844 - val_mean_absolute_error: 0.0844 - val_mean_squared_error: 0.0258\n",
      "Epoch 25: early stopping\n",
      "1600/1600 [==============================] - 2s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=max_trials_val,\n",
    "    directory='my_tuner_dir',\n",
    "    project_name='DFFNN_optimization_norm'\n",
    ")\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=1E-4)\n",
    "tuner.search(X_train_norm, y_train_norm, epochs=epochs_val, batch_size=batch_size_val, callbacks=[callback], validation_data=(X_val_norm, y_val_norm))\n",
    "tuner.results_summary()\n",
    "end_time = time.time()\n",
    "\n",
    "best_model = tuner.get_best_models()[0]\n",
    "print(\"Best Model: \", best_model)\n",
    "\n",
    "\n",
    "history = best_model.fit(X_train_norm, y_train_norm, epochs=epochs_val, batch_size=batch_size_val, validation_data=(X_val_norm, y_val_norm), callbacks=[callback])\n",
    "\n",
    "y_pred = best_model.predict(X_test_norm)\n",
    "#print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Valor Real  Predicciones\n",
      "0        0.128378      0.200324\n",
      "1        0.347826      0.279113\n",
      "2        0.051724      0.114578\n",
      "3        0.295400      0.244977\n",
      "4        0.155556      0.098535\n",
      "...           ...           ...\n",
      "51171    0.321429      0.132531\n",
      "51172    0.011936      0.031590\n",
      "51173    0.053030      0.104980\n",
      "51174    0.312155      0.252653\n",
      "51175    0.600000      0.462336\n",
      "\n",
      "[51176 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdrianGilGamboa\\AppData\\Local\\Temp\\ipykernel_17112\\1696851233.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[19.51019157 21.06286038 22.83313935 19.39220949 17.75914561 18.1616634\n",
      " 20.35502601 18.84101207 38.22458571 20.27732049 45.46156937 19.96335268\n",
      " 46.14192778 17.29043203 17.91836925 19.87818758 21.83925015]' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Column1  Column2  Column3  Column4  Column5  Column6  Column7  \\\n",
      "24905      58.0     66.0     62.0     56.0     77.0     60.0    170.0   \n",
      "155320    101.0     39.0     24.0     28.0     65.0     32.0     28.0   \n",
      "8745       30.0     30.0     29.0     31.0     31.0     31.0     29.0   \n",
      "60462     193.0     59.0     48.0     87.0    186.0     59.0     47.0   \n",
      "4241       35.0     29.0     37.0     31.0     29.0     32.0     32.0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "132964     37.0     35.0     33.0     35.0     34.0     36.0     21.0   \n",
      "130033      5.0      8.0      7.0     11.0      1.0      7.0      8.0   \n",
      "124375    137.0    120.0     63.0     60.0     54.0     73.0      6.0   \n",
      "50855     185.0     32.0     49.0     67.0     51.0     37.0     56.0   \n",
      "5442       29.0     65.0     44.0     31.0     50.0     69.0     48.0   \n",
      "\n",
      "        Column8  Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
      "24905      69.0     66.0      48.0      53.0  0.216216      41.0   2005/2   \n",
      "155320     45.0     57.0      35.0      27.0  0.186335      59.0   2006/3   \n",
      "8745       29.0     25.0      27.0      26.0  0.155172      23.0   2015/2   \n",
      "60462      52.0    162.0      43.0      36.0  0.053269     150.0   2011/3   \n",
      "4241       23.0     27.0      28.0     137.0  0.096296      34.0   2009/4   \n",
      "...         ...      ...       ...       ...       ...       ...      ...   \n",
      "132964     22.0     44.0      31.0      26.0  0.090909      65.5   2006/3   \n",
      "130033      6.0     12.0       8.0      11.0  0.007958      10.0   2002/4   \n",
      "124375     26.0     32.0      14.0      14.0  0.060606      13.0   2005/4   \n",
      "50855     159.0    109.0      56.0      70.0  0.908840     145.0   2012/4   \n",
      "5442       40.0     55.0      53.0      52.0  0.463636      85.0   2013/1   \n",
      "\n",
      "        Column15  Predicted_Column13  \n",
      "24905       8751           51.647930  \n",
      "155320     56704           47.937202  \n",
      "8745        3059           26.645502  \n",
      "60462      20496          129.175389  \n",
      "4241        1242           26.302183  \n",
      "...          ...                 ...  \n",
      "132964     46569           19.710875  \n",
      "130033     45459           24.818666  \n",
      "124375     43274           19.857371  \n",
      "50855      17695          123.460560  \n",
      "5442        1656           69.856937  \n",
      "\n",
      "[51176 rows x 16 columns]\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           51.647930\n",
      "155320      59.0   2006/3     56704           47.937202\n",
      "8745        23.0   2015/2      3059           26.645502\n",
      "60462      150.0   2011/3     20496          129.175389\n",
      "4241        34.0   2009/4      1242           26.302183\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           19.710875\n",
      "130033      10.0   2002/4     45459           24.818666\n",
      "124375      13.0   2005/4     43274           19.857371\n",
      "50855      145.0   2012/4     17695          123.460560\n",
      "5442        85.0   2013/1      1656           69.856937\n",
      "\n",
      "[51176 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame con las predicciones desnormalizadas y los valores reales\n",
    "resultados = pd.DataFrame({'Valor Real': y_test_norm.values.flatten(), 'Predicciones': y_pred.flatten()})\n",
    "print(resultados)\n",
    "\n",
    "# Agregar la columna de predicciones al conjunto de prueba\n",
    "test_data['Predicted_Column13'] = y_pred.flatten()\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados desnormalizados\n",
    "desnormalized_test_data = test_data.copy()\n",
    "\n",
    "# Desnormalizar 'Column1' a 'Column13' y 'Predicted_Column13' según la normalización por grupos\n",
    "for group, scalerY in scalers.items():\n",
    "    # Filtrar el conjunto de prueba correspondiente al grupo\n",
    "    group_test_data = test_data[test_data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas normalizadas para desnormalizar\n",
    "    normalized_features = group_test_data[['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']]\n",
    "\n",
    "    # Desnormalizar los datos utilizando el objeto scalerY correspondiente\n",
    "    original_data = scalerY.inverse_transform(normalized_features)\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar los datos desnormalizados\n",
    "    temp_df = pd.DataFrame(original_data, columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13'])\n",
    "\n",
    "    # Actualizar el DataFrame desnormalizado con los datos desnormalizados\n",
    "    desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la desnormalización\n",
    "print(desnormalized_test_data)\n",
    "\n",
    "# Eliminar todas las columnas excepto las últimas cuatro\n",
    "resultados = desnormalized_test_data.iloc[:, -4:]\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la eliminación de columnas\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_norm: 24905      41.0\n",
      "155320     59.0\n",
      "8745       23.0\n",
      "60462     150.0\n",
      "4241       34.0\n",
      "          ...  \n",
      "132964     65.5\n",
      "130033     10.0\n",
      "124375     13.0\n",
      "50855     145.0\n",
      "5442       85.0\n",
      "Name: Column13, Length: 51176, dtype: float64\n",
      "y_pred: 24905      51.647930\n",
      "155320     47.937202\n",
      "8745       26.645502\n",
      "60462     129.175389\n",
      "4241       26.302183\n",
      "             ...    \n",
      "132964     19.710875\n",
      "130033     24.818666\n",
      "124375     19.857371\n",
      "50855     123.460560\n",
      "5442       69.856937\n",
      "Name: Predicted_Column13, Length: 51176, dtype: float64\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           51.647930\n",
      "155320      59.0   2006/3     56704           47.937202\n",
      "8745        23.0   2015/2      3059           26.645502\n",
      "60462      150.0   2011/3     20496          129.175389\n",
      "4241        34.0   2009/4      1242           26.302183\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           19.710875\n",
      "130033      10.0   2002/4     45459           24.818666\n",
      "124375      13.0   2005/4     43274           19.857371\n",
      "50855      145.0   2012/4     17695          123.460560\n",
      "5442        85.0   2013/1      1656           69.856937\n",
      "\n",
      "[51176 rows x 4 columns]\n",
      "RMSE en el conjunto de prueba: 43.0152339290965\n",
      "MAE en el conjunto de prueba: 12.949707341167407\n",
      "Mean absolute percentage error (MAPE): 0.473320\n",
      "Tiempo de busqueda de parametros: 2007.63 segundos\n",
      "Tiempo de busqueda de parametros (HH:MM:SS): 0:33:27\n"
     ]
    }
   ],
   "source": [
    "# Obtener y_test_norm de la primera columna de resultados\n",
    "y_test_norm = resultados['Column13']\n",
    "\n",
    "# Obtener y_pred de la última columna del conjunto de prueba después de la desnormalización\n",
    "y_pred = desnormalized_test_data['Predicted_Column13']\n",
    "\n",
    "# Imprimir y_test_norm y y_pred\n",
    "print(\"y_test_norm:\", y_test_norm)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(resultados)\n",
    "\n",
    "\n",
    "# Calcular RMSE con datos desnormalizados\n",
    "rmse = np.sqrt(mean_squared_error(y_test_norm, y_pred))\n",
    "print(f'RMSE en el conjunto de prueba: {rmse}')\n",
    "\n",
    "# Calcular MAE con datos desnormalizados\n",
    "mae = mean_absolute_error(y_test_norm, y_pred)\n",
    "print(f'MAE en el conjunto de prueba: {mae}')\n",
    "\n",
    "#Calcular MAPE con datos desnormalizados\n",
    "print(\"Mean absolute percentage error (MAPE): %f\" % mean_absolute_percentage_error(y_test_norm, y_pred))\n",
    "\n",
    "# Calcula la duración del entrenamiento en segundos\n",
    "training_duration = end_time - start_time\n",
    "# Imprime el tiempo de entrenamiento en segundos y en formato de horas, minutos y segundos\n",
    "print(f'Tiempo de busqueda de parametros: {training_duration:.2f} segundos')\n",
    "print(f'Tiempo de busqueda de parametros (HH:MM:SS): {int(training_duration // 3600)}:{int((training_duration % 3600) // 60)}:{int(training_duration % 60)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el DataFrame en un archivo CSV\n",
    "resultados.to_csv(\"DFFNN_OPT_FineTuning_Norm(1-15iter).csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Año       Column13  Predicted_Column13  MAPE_fila\n",
      "0   2002  104265.678499        93749.297568  10.086139\n",
      "1   2003  177491.021691       149558.691977  15.737320\n",
      "2   2004  164849.403889       147850.363588  10.311860\n",
      "3   2005  159059.300841       141778.276056  10.864517\n",
      "4   2006  131381.875214       126489.871886   3.723499\n",
      "5   2007  131223.654627       120171.036909   8.422733\n",
      "6   2008  122996.287059       117915.661138   4.130715\n",
      "7   2009  128977.774184       112805.255847  12.538996\n",
      "8   2010  117483.995234       108581.078640   7.577982\n",
      "9   2011  118736.260722       112387.515086   5.346931\n",
      "10  2012  122697.561296       116997.898108   4.645295\n",
      "11  2013  109818.900063       107559.389368   2.057488\n",
      "12  2014  104937.636563       104072.861017   0.824085\n",
      "13  2015  109243.945117       105624.941568   3.312773\n",
      "Media de MAPE (por fila): 7.11%\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'Column14' para extraer el año\n",
    "resultados['Año'] = resultados['Column14'].str.split('/').str[0].astype(int)\n",
    "\n",
    "# Agrupar por año y calcular la suma de reales y predicciones\n",
    "suma_anual = resultados.groupby('Año').agg({\n",
    "    'Column13': 'sum',  # Suma de valores reales\n",
    "    'Predicted_Column13': 'sum'  # Suma de predicciones\n",
    "}).reset_index()\n",
    "\n",
    "# Agregar una columna de MAPE por fila\n",
    "suma_anual['MAPE_fila'] = (\n",
    "    (abs(suma_anual['Column13'] - suma_anual['Predicted_Column13']) / suma_anual['Column13']) * 100\n",
    ")\n",
    "\n",
    "# Calcular la media de la columna MAPE\n",
    "media_mape = suma_anual['MAPE_fila'].mean()\n",
    "\n",
    "# Mostrar el DataFrame actualizado y la media\n",
    "print(suma_anual)\n",
    "print(f\"Media de MAPE (por fila): {media_mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
