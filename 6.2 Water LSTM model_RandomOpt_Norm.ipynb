{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Cargar los datos\n",
    "data = pd.read_csv(\"../TransferLearning/Cluster0ReadyToNN.csv\", sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un diccionario para almacenar los objetos scaler por grupo\n",
    "scalers = {}\n",
    "\n",
    "# Iterar sobre los grupos únicos en Column15\n",
    "for group in data['Column15'].unique():\n",
    "    # Filtrar datos por grupo\n",
    "    group_data = data[data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas para normalización (las 13 primeras)\n",
    "    features = group_data.iloc[:, :13]\n",
    "\n",
    "    # Normalizar los datos con MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(features)\n",
    "\n",
    "    # Almacenar el scaler en el diccionario\n",
    "    scalers[group] = scaler\n",
    "\n",
    "    # Actualizar el DataFrame con los datos normalizados\n",
    "    data.loc[data['Column15'] == group, 'Column1':'Column13'] = normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83587 35823 12\n"
     ]
    }
   ],
   "source": [
    "# Ordenar el DataFrame por 'Column 14' de forma ascendente\n",
    "data = data.sort_values(by='Column14')\n",
    "\n",
    "# Dividir los datos en entrenamiento (70%) y temporal (30%)\n",
    "train_temp_data, test_data = train_test_split(data, test_size=0.3, stratify=data['Column15'], random_state=0)\n",
    "#train_temp_data, test_data = train_test_split(data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Dividir el temporal en entrenamiento (70%) y validación (30%)\n",
    "train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, stratify=train_temp_data['Column15'], random_state=0)\n",
    "#train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Separar características (X) y columna objetivo (y)\n",
    "X_train = train_data.iloc[:, :12]\n",
    "y_train = train_data['Column13']\n",
    "X_val = validation_data.iloc[:, :12]\n",
    "y_val = validation_data['Column13']\n",
    "X_test = test_data.iloc[:, :12]\n",
    "y_test = test_data['Column13']\n",
    "\n",
    "\n",
    "# Reshape de los datos para LSTM (número de muestras, número de pasos de tiempo, número de características)\n",
    "n_samples_train, n_features = X_train.shape\n",
    "n_samples_val = X_val.shape[0]\n",
    "n_timesteps = 1\n",
    "X_train = X_train.values.reshape(n_samples_train, n_timesteps, n_features)\n",
    "X_val = X_val.values.reshape(n_samples_val, n_timesteps, n_features)\n",
    "X_test = X_test.values.reshape(X_test.shape[0], n_timesteps, n_features)\n",
    "\n",
    "print(n_samples_train, n_samples_val, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función para construir el modelo\n",
    "def build_model(hp):\n",
    "    model = Sequential()\n",
    "\n",
    "    # Primera capa LSTM con input_shape y 12 neuronas fijas\n",
    "    model.add(LSTM(\n",
    "        units=12,  # Fijo en 12 neuronas\n",
    "        activation='tanh',\n",
    "        input_shape=(n_timesteps, n_features),  # Solo en la primera capa\n",
    "        return_sequences=True  # Para permitir capas ocultas adicionales\n",
    "    ))\n",
    "    \n",
    "    # Número de capas ocultas\n",
    "    num_layers = hp.Int('num_layers', 5, 15)\n",
    "    \n",
    "    # Capas ocultas LSTM con número de neuronas optimizado\n",
    "    for i in range(num_layers):\n",
    "        model.add(LSTM(\n",
    "            units=hp.Int(f'units_{i}', min_value=20, max_value=80, step=10),\n",
    "            activation='tanh',\n",
    "            return_sequences=(i < num_layers - 1)  # Solo True en capas intermedias\n",
    "        ))\n",
    "\n",
    "    # Capa de salida con 1 neurona\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "\n",
    "    # Optimizador y tasa de aprendizaje aleatoria\n",
    "    optimizer = Adam(\n",
    "    learning_rate=hp.Float('learning_rate', min_value=0.00001, max_value=0.001, sampling='log')\n",
    "    )\n",
    "    \n",
    "    model.compile(\n",
    "        loss='mean_absolute_error',\n",
    "        optimizer=optimizer,\n",
    "        metrics=['mean_absolute_error']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Params\n",
    "epochs_val = 100\n",
    "batch_size_val = 256\n",
    "max_trials_val = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 25 Complete [00h 02m 25s]\n",
      "val_loss: 0.13222603499889374\n",
      "\n",
      "Best val_loss So Far: 0.08526664972305298\n",
      "Total elapsed time: 03h 59m 26s\n"
     ]
    }
   ],
   "source": [
    "# Definir el hiperbuscador RandomSearch\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_loss',\n",
    "    max_trials=max_trials_val,  # Número de iteraciones\n",
    "    directory='my_tuner_dir',\n",
    "    project_name='LSTM_optimization_norm'\n",
    ")\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1, min_delta=1E-4)\n",
    "\n",
    "# Ejecutar la búsqueda de hiperparámetros\n",
    "tuner.search(X_train, y_train, epochs=epochs_val, batch_size=batch_size_val, validation_data=(X_val, y_val), callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 7s 3ms/step\n",
      "Mejores hiperparámetros encontrados:\n",
      " - Número de capas LSTM: 8\n",
      " - Unidades en LSTM 1: 30\n",
      " - Unidades en LSTM 2: 30\n",
      " - Unidades en LSTM 3: 20\n",
      " - Unidades en LSTM 4: 50\n",
      " - Unidades en LSTM 5: 20\n",
      " - Unidades en LSTM 6: 70\n",
      " - Unidades en LSTM 7: 40\n",
      " - Tasa de aprendizaje: 0.0005001588003685089\n"
     ]
    }
   ],
   "source": [
    "# Obtener el mejor modelo\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "# Predicciones con el mejor modelo\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Mostrar los mejores valores encontrados\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(f\" - Número de capas LSTM: {best_hps.get('num_layers')}\")\n",
    "for i in range(best_hps.get('num_layers') - 1):  # -1 porque la primera capa es fija\n",
    "    print(f\" - Unidades en LSTM {i + 1}: {best_hps.get(f'units_{i}')}\")\n",
    "print(f\" - Tasa de aprendizaje: {best_hps.get('learning_rate')}\")\n",
    "#print(f\" - Optimizador: {best_hps.get('optimizer')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "327/327 [==============================] - 21s 23ms/step - loss: 0.0855 - mean_absolute_error: 0.0855 - val_loss: 0.0852 - val_mean_absolute_error: 0.0852\n",
      "Epoch 2/100\n",
      "327/327 [==============================] - 4s 14ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - val_loss: 0.0853 - val_mean_absolute_error: 0.0853\n",
      "Epoch 3/100\n",
      "327/327 [==============================] - 5s 14ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - val_loss: 0.0854 - val_mean_absolute_error: 0.0854\n",
      "Epoch 4/100\n",
      "327/327 [==============================] - 5s 14ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - val_loss: 0.0861 - val_mean_absolute_error: 0.0861\n",
      "Epoch 5/100\n",
      "327/327 [==============================] - 5s 14ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - val_loss: 0.0851 - val_mean_absolute_error: 0.0851\n",
      "Epoch 6/100\n",
      "327/327 [==============================] - 4s 14ms/step - loss: 0.0853 - mean_absolute_error: 0.0853 - val_loss: 0.0854 - val_mean_absolute_error: 0.0854\n",
      "Epoch 7/100\n",
      "327/327 [==============================] - 4s 14ms/step - loss: 0.0853 - mean_absolute_error: 0.0853 - val_loss: 0.0855 - val_mean_absolute_error: 0.0855\n",
      "Epoch 8/100\n",
      "327/327 [==============================] - 4s 14ms/step - loss: 0.0853 - mean_absolute_error: 0.0853 - val_loss: 0.0854 - val_mean_absolute_error: 0.0854\n",
      "Epoch 9/100\n",
      "327/327 [==============================] - 4s 14ms/step - loss: 0.0853 - mean_absolute_error: 0.0853 - val_loss: 0.0858 - val_mean_absolute_error: 0.0858\n",
      "Epoch 10/100\n",
      "327/327 [==============================] - 4s 13ms/step - loss: 0.0853 - mean_absolute_error: 0.0853 - val_loss: 0.0852 - val_mean_absolute_error: 0.0852\n",
      "Epoch 11/100\n",
      "327/327 [==============================] - 5s 14ms/step - loss: 0.0852 - mean_absolute_error: 0.0852 - val_loss: 0.0856 - val_mean_absolute_error: 0.0856\n",
      "Epoch 11: early stopping\n",
      "Tiempo de entrenamiento del mejor modelo: 66.07 segundos\n",
      "1600/1600 [==============================] - 4s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning del mejor modelo\n",
    "start_time = time.time()\n",
    "history = best_model.fit(X_train, y_train, epochs=epochs_val, batch_size=batch_size_val, validation_data=(X_val, y_val), callbacks=[callback])\n",
    "end_time = time.time()\n",
    "\n",
    "# Calcular el tiempo total\n",
    "training_time = end_time - start_time\n",
    "print(f\"Tiempo de entrenamiento del mejor modelo: {training_time:.2f} segundos\")\n",
    "\n",
    "# Predicciones con el mejor modelo\n",
    "y_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Valor Real  Predicciones\n",
      "0            41.0      0.217384\n",
      "1            59.0      0.299282\n",
      "2            23.0      0.125546\n",
      "3           150.0      0.222728\n",
      "4            34.0      0.065877\n",
      "...           ...           ...\n",
      "51171        65.5      0.143755\n",
      "51172        10.0      0.043426\n",
      "51173        13.0      0.097179\n",
      "51174       145.0      0.244147\n",
      "51175        85.0      0.480339\n",
      "\n",
      "[51176 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdrianGilGamboa\\AppData\\Local\\Temp\\ipykernel_10196\\619335387.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[18.76637616 21.11667967 23.04028974 19.22304361 18.67889026 20.03488643\n",
      " 19.56176153 19.04939566 42.50346315 19.76826424 46.49299765 20.15648295\n",
      " 44.58092272 17.125563   17.83071336 19.63831144 23.54635106]' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Column1  Column2  Column3  Column4  Column5  Column6  Column7  \\\n",
      "24905      58.0     66.0     62.0     56.0     77.0     60.0    170.0   \n",
      "155320    101.0     39.0     24.0     28.0     65.0     32.0     28.0   \n",
      "8745       30.0     30.0     29.0     31.0     31.0     31.0     29.0   \n",
      "60462     193.0     59.0     48.0     87.0    186.0     59.0     47.0   \n",
      "4241       35.0     29.0     37.0     31.0     29.0     32.0     32.0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "132964     37.0     35.0     33.0     35.0     34.0     36.0     21.0   \n",
      "130033      5.0      8.0      7.0     11.0      1.0      7.0      8.0   \n",
      "124375    137.0    120.0     63.0     60.0     54.0     73.0      6.0   \n",
      "50855     185.0     32.0     49.0     67.0     51.0     37.0     56.0   \n",
      "5442       29.0     65.0     44.0     31.0     50.0     69.0     48.0   \n",
      "\n",
      "        Column8  Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
      "24905      69.0     66.0      48.0      53.0  0.216216      41.0   2005/2   \n",
      "155320     45.0     57.0      35.0      27.0  0.186335      59.0   2006/3   \n",
      "8745       29.0     25.0      27.0      26.0  0.155172      23.0   2015/2   \n",
      "60462      52.0    162.0      43.0      36.0  0.053269     150.0   2011/3   \n",
      "4241       23.0     27.0      28.0     137.0  0.096296      34.0   2009/4   \n",
      "...         ...      ...       ...       ...       ...       ...      ...   \n",
      "132964     22.0     44.0      31.0      26.0  0.090909      65.5   2006/3   \n",
      "130033      6.0     12.0       8.0      11.0  0.007958      10.0   2002/4   \n",
      "124375     26.0     32.0      14.0      14.0  0.060606      13.0   2005/4   \n",
      "50855     159.0    109.0      56.0      70.0  0.908840     145.0   2012/4   \n",
      "5442       40.0     55.0      53.0      52.0  0.463636      85.0   2013/1   \n",
      "\n",
      "        Column15  Predicted_Column13  \n",
      "24905       8751           54.172827  \n",
      "155320     56704           51.184366  \n",
      "8745        3059           27.281676  \n",
      "60462      20496          119.986633  \n",
      "4241        1242           21.893420  \n",
      "...          ...                 ...  \n",
      "132964     46569           20.025136  \n",
      "130033     45459           33.743544  \n",
      "124375     43274           18.827602  \n",
      "50855      17695          120.381253  \n",
      "5442        1656           71.837299  \n",
      "\n",
      "[51176 rows x 16 columns]\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           54.172827\n",
      "155320      59.0   2006/3     56704           51.184366\n",
      "8745        23.0   2015/2      3059           27.281676\n",
      "60462      150.0   2011/3     20496          119.986633\n",
      "4241        34.0   2009/4      1242           21.893420\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           20.025136\n",
      "130033      10.0   2002/4     45459           33.743544\n",
      "124375      13.0   2005/4     43274           18.827602\n",
      "50855      145.0   2012/4     17695          120.381253\n",
      "5442        85.0   2013/1      1656           71.837299\n",
      "\n",
      "[51176 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame con las predicciones desnormalizadas y los valores reales\n",
    "resultados = pd.DataFrame({'Valor Real': y_test.values.flatten(), 'Predicciones': y_pred.flatten()})\n",
    "print(resultados)\n",
    "\n",
    "# Agregar la columna de predicciones al conjunto de prueba\n",
    "test_data['Predicted_Column13'] = y_pred.flatten()\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados desnormalizados\n",
    "desnormalized_test_data = test_data.copy()\n",
    "\n",
    "# Desnormalizar 'Column1' a 'Column13' y 'Predicted_Column13' según la normalización por grupos\n",
    "for group, scalerY in scalers.items():\n",
    "    # Filtrar el conjunto de prueba correspondiente al grupo\n",
    "    group_test_data = test_data[test_data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas normalizadas para desnormalizar\n",
    "    normalized_features = group_test_data[['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']]\n",
    "\n",
    "    # Desnormalizar los datos utilizando el objeto scalerY correspondiente\n",
    "    original_data = scalerY.inverse_transform(normalized_features)\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar los datos desnormalizados\n",
    "    temp_df = pd.DataFrame(original_data, columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13'])\n",
    "\n",
    "    # Actualizar el DataFrame desnormalizado con los datos desnormalizados\n",
    "    desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la desnormalización\n",
    "print(desnormalized_test_data)\n",
    "\n",
    "# Eliminar todas las columnas excepto las últimas cuatro\n",
    "resultados = desnormalized_test_data.iloc[:, -4:]\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la eliminación de columnas\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test: 24905      41.0\n",
      "155320     59.0\n",
      "8745       23.0\n",
      "60462     150.0\n",
      "4241       34.0\n",
      "          ...  \n",
      "132964     65.5\n",
      "130033     10.0\n",
      "124375     13.0\n",
      "50855     145.0\n",
      "5442       85.0\n",
      "Name: Column13, Length: 51176, dtype: float64\n",
      "y_pred: 24905      54.172827\n",
      "155320     51.184366\n",
      "8745       27.281676\n",
      "60462     119.986633\n",
      "4241       21.893420\n",
      "             ...    \n",
      "132964     20.025136\n",
      "130033     33.743544\n",
      "124375     18.827602\n",
      "50855     120.381253\n",
      "5442       71.837299\n",
      "Name: Predicted_Column13, Length: 51176, dtype: float64\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           54.172827\n",
      "155320      59.0   2006/3     56704           51.184366\n",
      "8745        23.0   2015/2      3059           27.281676\n",
      "60462      150.0   2011/3     20496          119.986633\n",
      "4241        34.0   2009/4      1242           21.893420\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           20.025136\n",
      "130033      10.0   2002/4     45459           33.743544\n",
      "124375      13.0   2005/4     43274           18.827602\n",
      "50855      145.0   2012/4     17695          120.381253\n",
      "5442        85.0   2013/1      1656           71.837299\n",
      "\n",
      "[51176 rows x 4 columns]\n",
      "RMSE en el conjunto de prueba: 43.082494717213535\n",
      "MAE en el conjunto de prueba: 13.199875734952574\n",
      "MAPE en el conjunto de prueba: 0.5041425984408656\n",
      "Tiempo de entrenamiento: 66.07 segundos\n"
     ]
    }
   ],
   "source": [
    "# Obtener y_test_norm de la primera columna de resultados\n",
    "y_test = resultados['Column13']\n",
    "\n",
    "# Obtener y_pred de la última columna del conjunto de prueba después de la desnormalización\n",
    "y_pred = desnormalized_test_data['Predicted_Column13']\n",
    "\n",
    "# Imprimir y_test_norm y y_pred\n",
    "print(\"y_test:\", y_test)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(resultados)\n",
    "\n",
    "# Calcular RMSE con datos desnormalizados\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f'RMSE en el conjunto de prueba: {rmse}')\n",
    "\n",
    "# Calcular MAE con datos desnormalizados\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f'MAE en el conjunto de prueba: {mae}')\n",
    "\n",
    "# Calcular MRE con datos desnormalizados\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "print(f'MAPE en el conjunto de prueba: {mape}')\n",
    "\n",
    "# Calcula la duración del entrenamiento en segundos\n",
    "training_duration = end_time - start_time\n",
    "# Imprime el tiempo de entrenamiento en segundos y en formato de horas, minutos y segundos\n",
    "print(f'Tiempo de entrenamiento: {training_duration:.2f} segundos')\n",
    "#print(f'Tiempo de entrenamiento (HH:MM:SS): {int(training_duration // 3600)}:{int((training_duration % 3600) // 60)}:{int(training_duration % 60)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
