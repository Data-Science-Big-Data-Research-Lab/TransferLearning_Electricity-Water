{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def custom_mre(y_true, y_pred):\n",
    "    \"\"\"numerator = tf.reduce_mean(tf.abs(y_pred - y_true), axis=None)\n",
    "    denominator = tf.reduce_mean(tf.abs(y_true), axis=None)\n",
    "    mre = 100.0 * numerator / denominator\n",
    "    return mre\"\"\"\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    relative_error = np.abs((y_true - y_pred) / y_true)\n",
    "    \n",
    "    mre = np.mean(relative_error) * 100.0\n",
    "    \n",
    "    return mre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con punto y coma como delimitador\n",
    "data = pd.read_csv('Cluster0ReadyToNN.csv', delimiter=';')\n",
    "\n",
    "# Crear un diccionario para almacenar los objetos scaler por grupo\n",
    "scalers = {}\n",
    "\n",
    "# Iterar sobre los grupos únicos en Column15\n",
    "for group in data['Column15'].unique():\n",
    "    # Filtrar datos por grupo\n",
    "    group_data = data[data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas para normalización (las 13 primeras)\n",
    "    features = group_data.iloc[:, :13]\n",
    "\n",
    "    # Normalizar los datos con MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(features)\n",
    "\n",
    "    # Almacenar el scaler en el diccionario\n",
    "    scalers[group] = scaler\n",
    "\n",
    "    # Actualizar el DataFrame con los datos normalizados\n",
    "    data.loc[data['Column15'] == group, 'Column1':'Column13'] = normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar el DataFrame por 'Column 14' de forma ascendente\n",
    "data = data.sort_values(by='Column14')\n",
    "\n",
    "# Dividir los datos en entrenamiento (70%) y temporal (30%)\n",
    "train_temp_data, test_data = train_test_split(data, test_size=0.3, stratify=data['Column15'], random_state=0)\n",
    "#train_temp_data, test_data = train_test_split(data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Dividir el temporal en entrenamiento (70%) y validación (30%)\n",
    "train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, stratify=train_temp_data['Column15'], random_state=0)\n",
    "#train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Separar características (X) y columna objetivo (y)\n",
    "X_train_norm = train_data.iloc[:, :12]\n",
    "y_train_norm = train_data['Column13']\n",
    "X_val_norm = validation_data.iloc[:, :12]\n",
    "y_val_norm = validation_data['Column13']\n",
    "X_test_norm = test_data.iloc[:, :12]\n",
    "y_test_norm = test_data['Column13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_26 (Dense)            (None, 40)                520       \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 60)                2460      \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 80)                4880      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 80)                6480      \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 70)                5670      \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 30)                2130      \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 70)                2170      \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 70)                4970      \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 30)                2130      \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 70)                2170      \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 20)                1420      \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 60)                1260      \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 36321 (141.88 KB)\n",
      "Trainable params: 36321 (141.88 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo de la red neuronal\n",
    "model = keras.Sequential()\n",
    "\n",
    "# Agregar capa de entrada\n",
    "model.add(keras.layers.Input(shape=(12,)))  # Ahora son 12 características\n",
    "\n",
    "# Agregar capas ocultas con las neuronas especificadas\n",
    "neuronas_ocultas = [40,60,80,80,70,30,70,70,30,70,20,60]\n",
    "for neurons in neuronas_ocultas:\n",
    "    model.add(keras.layers.Dense(neurons, activation='tanh'))\n",
    "\n",
    "# Agregar capa de salida con 1 neurona\n",
    "model.add(keras.layers.Dense(1, activation='tanh'))\n",
    "\n",
    "# Definir el optimizador\n",
    "optimizer = keras.optimizers.Nadam(epsilon=1E-8, learning_rate=0.000044)\n",
    "\n",
    "# Compilar el modelo con el optimizador personalizado\n",
    "model.compile(optimizer=optimizer, loss='mse', metrics=['mse'])\n",
    "\n",
    "# Resumen del modelo\n",
    "model.summary()\n",
    "\n",
    "# Configurar Early Stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',  # Métrica a monitorear (en este caso, la pérdida en el conjunto de validación)\n",
    "    patience=20,  # Número de épocas sin mejora antes de detener el entrenamiento\n",
    "    restore_best_weights=True  # Restaurar los mejores pesos del modelo cuando se detiene\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "327/327 [==============================] - 5s 5ms/step - loss: 0.0321 - mse: 0.0321 - val_loss: 0.0287 - val_mse: 0.0287\n",
      "Epoch 2/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0278 - mse: 0.0278 - val_loss: 0.0274 - val_mse: 0.0274\n",
      "Epoch 3/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0270 - mse: 0.0270 - val_loss: 0.0271 - val_mse: 0.0271\n",
      "Epoch 4/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0267 - mse: 0.0267 - val_loss: 0.0266 - val_mse: 0.0266\n",
      "Epoch 5/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0264 - mse: 0.0264 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 6/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0262 - mse: 0.0262 - val_loss: 0.0264 - val_mse: 0.0264\n",
      "Epoch 7/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0260 - mse: 0.0260 - val_loss: 0.0261 - val_mse: 0.0261\n",
      "Epoch 8/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0259 - mse: 0.0259 - val_loss: 0.0263 - val_mse: 0.0263\n",
      "Epoch 9/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0258 - mse: 0.0258 - val_loss: 0.0259 - val_mse: 0.0259\n",
      "Epoch 10/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0257 - mse: 0.0257 - val_loss: 0.0258 - val_mse: 0.0258\n",
      "Epoch 11/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 12/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0256 - mse: 0.0256 - val_loss: 0.0259 - val_mse: 0.0259\n",
      "Epoch 13/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0255 - mse: 0.0255 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 14/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 15/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 16/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0254 - mse: 0.0254 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 17/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 18/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0257 - val_mse: 0.0257\n",
      "Epoch 19/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 20/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0253 - mse: 0.0253 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 21/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 22/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 23/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 24/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 25/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 26/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0252 - mse: 0.0252 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 27/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 28/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 29/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0256 - val_mse: 0.0256\n",
      "Epoch 30/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 31/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 32/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0251 - mse: 0.0251 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 33/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0255 - val_mse: 0.0255\n",
      "Epoch 34/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 35/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 36/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 37/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 38/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0250 - mse: 0.0250 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 39/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 40/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 41/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 42/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 43/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 44/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 45/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 46/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0249 - mse: 0.0249 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 47/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 48/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0254 - val_mse: 0.0254\n",
      "Epoch 49/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 50/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 51/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 52/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 53/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 54/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 55/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 56/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 57/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 58/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 59/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 60/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0253 - val_mse: 0.0253\n",
      "Epoch 61/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 62/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0248 - mse: 0.0248 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 63/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 64/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 65/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 66/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 67/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 68/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 69/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 70/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 71/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 72/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 73/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 74/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 75/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 76/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 77/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 78/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 79/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 80/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 81/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 82/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 83/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 84/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0247 - mse: 0.0247 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 85/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 86/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 87/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 88/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 89/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 90/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 91/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 92/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 93/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 94/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 95/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 96/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 97/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 98/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 99/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 100/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 101/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 102/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 103/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 104/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 105/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 106/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 107/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 108/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 109/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 110/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 111/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 112/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 113/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 114/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0246 - mse: 0.0246 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 115/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 116/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 117/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 118/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 119/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0251 - val_mse: 0.0251\n",
      "Epoch 120/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 121/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 122/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 123/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 124/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0252 - val_mse: 0.0252\n",
      "Epoch 125/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 126/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 127/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 128/300\n",
      "327/327 [==============================] - 1s 4ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "Epoch 129/300\n",
      "327/327 [==============================] - 1s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 130/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0250 - val_mse: 0.0250\n",
      "Epoch 131/300\n",
      "327/327 [==============================] - 2s 5ms/step - loss: 0.0245 - mse: 0.0245 - val_loss: 0.0249 - val_mse: 0.0249\n",
      "1600/1600 [==============================] - 2s 1ms/step\n",
      "[[0.22602494]\n",
      " [0.30800417]\n",
      " [0.14398377]\n",
      " ...\n",
      " [0.08880278]\n",
      " [0.30060017]\n",
      " [0.5075897 ]]\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo con Early Stopping\n",
    "epochs = 300  \n",
    "batch_size = 256\n",
    "# Comienza a medir el tiempo de entrenamiento\n",
    "start_time = time.time()\n",
    "#Entrenamiento del modelo\n",
    "history = model.fit(X_train_norm, y_train_norm, epochs=epochs, batch_size=batch_size, validation_data=(X_val_norm, y_val_norm), callbacks=[early_stopping])\n",
    "# Finaliza la medición del tiempo de entrenamiento\n",
    "end_time = time.time()\n",
    "\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(X_test_norm)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Valor Real  Predicciones\n",
      "0            41.0      0.226025\n",
      "1            59.0      0.308004\n",
      "2            23.0      0.143984\n",
      "3           150.0      0.265041\n",
      "4            34.0      0.101969\n",
      "...           ...           ...\n",
      "51171        65.5      0.168927\n",
      "51172        10.0      0.071212\n",
      "51173        13.0      0.088803\n",
      "51174       145.0      0.300600\n",
      "51175        85.0      0.507590\n",
      "\n",
      "[51176 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdrianGilGamboa\\AppData\\Local\\Temp\\ipykernel_19808\\1696851233.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[20.71956566 22.21303873 23.12504978 19.9679786  22.22203369 20.77547765\n",
      " 20.765049   20.11592598 40.06992191 20.41897523 43.71335185 20.75099632\n",
      " 42.32951242 17.82632804 18.62661885 19.97764014 23.80254489]' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Column1  Column2  Column3  Column4  Column5  Column6  Column7  \\\n",
      "24905      58.0     66.0     62.0     56.0     77.0     60.0    170.0   \n",
      "155320    101.0     39.0     24.0     28.0     65.0     32.0     28.0   \n",
      "8745       30.0     30.0     29.0     31.0     31.0     31.0     29.0   \n",
      "60462     193.0     59.0     48.0     87.0    186.0     59.0     47.0   \n",
      "4241       35.0     29.0     37.0     31.0     29.0     32.0     32.0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "132964     37.0     35.0     33.0     35.0     34.0     36.0     21.0   \n",
      "130033      5.0      8.0      7.0     11.0      1.0      7.0      8.0   \n",
      "124375    137.0    120.0     63.0     60.0     54.0     73.0      6.0   \n",
      "50855     185.0     32.0     49.0     67.0     51.0     37.0     56.0   \n",
      "5442       29.0     65.0     44.0     31.0     50.0     69.0     48.0   \n",
      "\n",
      "        Column8  Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
      "24905      69.0     66.0      48.0      53.0  0.216216      41.0   2005/2   \n",
      "155320     45.0     57.0      35.0      27.0  0.186335      59.0   2006/3   \n",
      "8745       29.0     25.0      27.0      26.0  0.155172      23.0   2015/2   \n",
      "60462      52.0    162.0      43.0      36.0  0.053269     150.0   2011/3   \n",
      "4241       23.0     27.0      28.0     137.0  0.096296      34.0   2009/4   \n",
      "...         ...      ...       ...       ...       ...       ...      ...   \n",
      "132964     22.0     44.0      31.0      26.0  0.090909      65.5   2006/3   \n",
      "130033      6.0     12.0       8.0      11.0  0.007958      10.0   2002/4   \n",
      "124375     26.0     32.0      14.0      14.0  0.060606      13.0   2005/4   \n",
      "50855     159.0    109.0      56.0      70.0  0.908840     145.0   2012/4   \n",
      "5442       40.0     55.0      53.0      52.0  0.463636      85.0   2013/1   \n",
      "\n",
      "        Column15  Predicted_Column13  \n",
      "24905       8751           55.451691  \n",
      "155320     56704           52.588671  \n",
      "8745        3059           28.351058  \n",
      "60462      20496          137.462078  \n",
      "4241        1242           26.765779  \n",
      "...          ...                 ...  \n",
      "132964     46569           20.729962  \n",
      "130033     45459           54.694163  \n",
      "124375     43274           17.721967  \n",
      "50855      17695          140.817262  \n",
      "5442        1656           74.834867  \n",
      "\n",
      "[51176 rows x 16 columns]\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           55.451691\n",
      "155320      59.0   2006/3     56704           52.588671\n",
      "8745        23.0   2015/2      3059           28.351058\n",
      "60462      150.0   2011/3     20496          137.462078\n",
      "4241        34.0   2009/4      1242           26.765779\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           20.729962\n",
      "130033      10.0   2002/4     45459           54.694163\n",
      "124375      13.0   2005/4     43274           17.721967\n",
      "50855      145.0   2012/4     17695          140.817262\n",
      "5442        85.0   2013/1      1656           74.834867\n",
      "\n",
      "[51176 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame con las predicciones desnormalizadas y los valores reales\n",
    "resultados = pd.DataFrame({'Valor Real': y_test_norm.values.flatten(), 'Predicciones': y_pred.flatten()})\n",
    "print(resultados)\n",
    "\n",
    "# Agregar la columna de predicciones al conjunto de prueba\n",
    "test_data['Predicted_Column13'] = y_pred.flatten()\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados desnormalizados\n",
    "desnormalized_test_data = test_data.copy()\n",
    "\n",
    "# Desnormalizar 'Column1' a 'Column13' y 'Predicted_Column13' según la normalización por grupos\n",
    "for group, scalerY in scalers.items():\n",
    "    # Filtrar el conjunto de prueba correspondiente al grupo\n",
    "    group_test_data = test_data[test_data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas normalizadas para desnormalizar\n",
    "    normalized_features = group_test_data[['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']]\n",
    "\n",
    "    # Desnormalizar los datos utilizando el objeto scalerY correspondiente\n",
    "    original_data = scalerY.inverse_transform(normalized_features)\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar los datos desnormalizados\n",
    "    temp_df = pd.DataFrame(original_data, columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13'])\n",
    "\n",
    "    # Actualizar el DataFrame desnormalizado con los datos desnormalizados\n",
    "    desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la desnormalización\n",
    "print(desnormalized_test_data)\n",
    "\n",
    "# Eliminar todas las columnas excepto las últimas cuatro\n",
    "resultados = desnormalized_test_data.iloc[:, -4:]\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la eliminación de columnas\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_norm: 24905      41.0\n",
      "155320     59.0\n",
      "8745       23.0\n",
      "60462     150.0\n",
      "4241       34.0\n",
      "          ...  \n",
      "132964     65.5\n",
      "130033     10.0\n",
      "124375     13.0\n",
      "50855     145.0\n",
      "5442       85.0\n",
      "Name: Column13, Length: 51176, dtype: float64\n",
      "y_pred: 24905      55.451691\n",
      "155320     52.588671\n",
      "8745       28.351058\n",
      "60462     137.462078\n",
      "4241       26.765779\n",
      "             ...    \n",
      "132964     20.729962\n",
      "130033     54.694163\n",
      "124375     17.721967\n",
      "50855     140.817262\n",
      "5442       74.834867\n",
      "Name: Predicted_Column13, Length: 51176, dtype: float64\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           55.451691\n",
      "155320      59.0   2006/3     56704           52.588671\n",
      "8745        23.0   2015/2      3059           28.351058\n",
      "60462      150.0   2011/3     20496          137.462078\n",
      "4241        34.0   2009/4      1242           26.765779\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           20.729962\n",
      "130033      10.0   2002/4     45459           54.694163\n",
      "124375      13.0   2005/4     43274           17.721967\n",
      "50855      145.0   2012/4     17695          140.817262\n",
      "5442        85.0   2013/1      1656           74.834867\n",
      "\n",
      "[51176 rows x 4 columns]\n",
      "RMSE en el conjunto de prueba: 43.30\n",
      "MAE en el conjunto de prueba: 15.06\n",
      "Mean absolute percentage error (MAPE): 0.667088\n",
      "Tiempo de entrenamiento: 194.45 segundos\n",
      "Tiempo de entrenamiento (HH:MM:SS): 0:3:14\n"
     ]
    }
   ],
   "source": [
    "# Obtener y_test_norm de la primera columna de resultados\n",
    "y_test_norm = resultados['Column13']\n",
    "\n",
    "# Obtener y_pred de la última columna del conjunto de prueba después de la desnormalización\n",
    "y_pred = desnormalized_test_data['Predicted_Column13']\n",
    "\n",
    "# Imprimir y_test_norm y y_pred\n",
    "print(\"y_test_norm:\", y_test_norm)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(resultados)\n",
    "# Guardar el DataFrame resultados en un archivo CSV\n",
    "resultados.to_csv('resultadosWaterKNIMENorm12layersTanh_MSE.csv', index=False)\n",
    "\n",
    "# Calcular RMSE con datos desnormalizados\n",
    "rmse = np.sqrt(mean_squared_error(y_test_norm, y_pred))\n",
    "print(f'RMSE en el conjunto de prueba: {rmse:.2f}')\n",
    "\n",
    "# Calcular MAE con datos desnormalizados\n",
    "mae = mean_absolute_error(y_test_norm, y_pred)\n",
    "print(f'MAE en el conjunto de prueba: {mae:.2f}')\n",
    "\n",
    "#Calcular MAPE con datos desnormalizados\n",
    "print(\"Mean absolute percentage error (MAPE): %f\" % mean_absolute_percentage_error(y_test_norm, y_pred))\n",
    "\n",
    "# Calcula la duración del entrenamiento en segundos\n",
    "training_duration = end_time - start_time\n",
    "# Imprime el tiempo de entrenamiento en segundos y en formato de horas, minutos y segundos\n",
    "print(f'Tiempo de entrenamiento: {training_duration:.2f} segundos')\n",
    "print(f'Tiempo de entrenamiento (HH:MM:SS): {int(training_duration // 3600)}:{int((training_duration % 3600) // 60)}:{int(training_duration % 60)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Año       Column13  Predicted_Column13  MAPE_fila\n",
      "0   2002  104265.678499       101239.084875   2.902771\n",
      "1   2003  177491.021691       166279.109569   6.316890\n",
      "2   2004  164849.403889       163826.212655   0.620682\n",
      "3   2005  159059.300841       158585.406181   0.297936\n",
      "4   2006  131381.875214       142057.003440   8.125267\n",
      "5   2007  131223.654627       137741.803255   4.967206\n",
      "6   2008  122996.287059       135242.151161   9.956288\n",
      "7   2009  128977.774184       131368.953769   1.853947\n",
      "8   2010  117483.995234       125947.126567   7.203646\n",
      "9   2011  118736.260722       130746.369651  10.114946\n",
      "10  2012  122697.561296       135470.196498  10.409853\n",
      "11  2013  109818.900063       125413.487569  14.200277\n",
      "12  2014  104937.636563       121247.936305  15.542850\n",
      "13  2015  109243.945117       123272.331669  12.841340\n",
      "Media de MAPE (por fila): 7.53%\n"
     ]
    }
   ],
   "source": [
    "# Convertir la columna 'Column14' para extraer el año\n",
    "resultados['Año'] = resultados['Column14'].str.split('/').str[0].astype(int)\n",
    "\n",
    "# Agrupar por año y calcular la suma de reales y predicciones\n",
    "suma_anual = resultados.groupby('Año').agg({\n",
    "    'Column13': 'sum',  # Suma de valores reales\n",
    "    'Predicted_Column13': 'sum'  # Suma de predicciones\n",
    "}).reset_index()\n",
    "\n",
    "# Agregar una columna de MAPE por fila\n",
    "suma_anual['MAPE_fila'] = (\n",
    "    (abs(suma_anual['Column13'] - suma_anual['Predicted_Column13']) / suma_anual['Column13']) * 100\n",
    ")\n",
    "\n",
    "# Calcular la media de la columna MAPE\n",
    "media_mape = suma_anual['MAPE_fila'].mean()\n",
    "\n",
    "# Mostrar el DataFrame actualizado y la media\n",
    "print(suma_anual)\n",
    "print(f\"Media de MAPE (por fila): {media_mape:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
