{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def custom_mre(y_true, y_pred):\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "    relative_error = np.abs((y_true - y_pred) / y_true)\n",
    "    mre = np.mean(relative_error) * 100.0\n",
    "    return mre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el archivo CSV con punto y coma como delimitador\n",
    "data = pd.read_csv('Cluster0ReadyToNN.csv', delimiter=';')\n",
    "\n",
    "# Crear un diccionario para almacenar los objetos scaler por grupo\n",
    "scalers = {}\n",
    "\n",
    "# Iterar sobre los grupos únicos en Column15\n",
    "for group in data['Column15'].unique():\n",
    "    # Filtrar datos por grupo\n",
    "    group_data = data[data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas para normalización (las 13 primeras)\n",
    "    features = group_data.iloc[:, :13]\n",
    "\n",
    "    # Normalizar los datos con MinMaxScaler\n",
    "    scaler = MinMaxScaler()\n",
    "    normalized_data = scaler.fit_transform(features)\n",
    "\n",
    "    # Almacenar el scaler en el diccionario\n",
    "    scalers[group] = scaler\n",
    "\n",
    "    # Actualizar el DataFrame con los datos normalizados\n",
    "    data.loc[data['Column15'] == group, 'Column1':'Column13'] = normalized_data\n",
    "\n",
    "# Guardar el DataFrame modificado en un nuevo archivo CSV con punto y coma como delimitador\n",
    "#data.to_csv('DataNormalized.csv', index=False, sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar el DataFrame por 'Column 14' de forma ascendente\n",
    "data = data.sort_values(by='Column14')\n",
    "\n",
    "# Dividir los datos en entrenamiento (70%) y temporal (30%)\n",
    "train_temp_data, test_data = train_test_split(data, test_size=0.3, stratify=data['Column15'], random_state=0)\n",
    "#train_temp_data, test_data = train_test_split(data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Dividir el temporal en entrenamiento (70%) y validación (30%)\n",
    "train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, stratify=train_temp_data['Column15'], random_state=0)\n",
    "#train_data, validation_data = train_test_split(train_temp_data, test_size=0.3, shuffle=False, random_state=0)\n",
    "\n",
    "# Separar características (X) y columna objetivo (y)\n",
    "X_train_norm = train_data.iloc[:, :12]\n",
    "y_train_norm = train_data['Column13']\n",
    "X_val_norm = validation_data.iloc[:, :12]\n",
    "y_val_norm = validation_data['Column13']\n",
    "X_test_norm = test_data.iloc[:, :12]\n",
    "y_test_norm = test_data['Column13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "      <th>Column4</th>\n",
       "      <th>Column5</th>\n",
       "      <th>Column6</th>\n",
       "      <th>Column7</th>\n",
       "      <th>Column8</th>\n",
       "      <th>Column9</th>\n",
       "      <th>Column10</th>\n",
       "      <th>Column11</th>\n",
       "      <th>Column12</th>\n",
       "      <th>Column13</th>\n",
       "      <th>Column14</th>\n",
       "      <th>Column15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.317757</td>\n",
       "      <td>0.163551</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.957447</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.979167</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.641026</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30078</th>\n",
       "      <td>0.164179</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.138686</td>\n",
       "      <td>0.109489</td>\n",
       "      <td>0.167883</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.160584</td>\n",
       "      <td>0.182482</td>\n",
       "      <td>0.153285</td>\n",
       "      <td>0.145985</td>\n",
       "      <td>0.167883</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>10755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139428</th>\n",
       "      <td>0.042553</td>\n",
       "      <td>0.652482</td>\n",
       "      <td>0.836879</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.816901</td>\n",
       "      <td>0.295775</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.492958</td>\n",
       "      <td>0.985915</td>\n",
       "      <td>0.436620</td>\n",
       "      <td>0.281690</td>\n",
       "      <td>0.535211</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>49541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30024</th>\n",
       "      <td>0.203822</td>\n",
       "      <td>0.286624</td>\n",
       "      <td>0.159236</td>\n",
       "      <td>0.197452</td>\n",
       "      <td>0.178344</td>\n",
       "      <td>0.197452</td>\n",
       "      <td>0.140127</td>\n",
       "      <td>0.159236</td>\n",
       "      <td>0.210191</td>\n",
       "      <td>0.267516</td>\n",
       "      <td>0.248408</td>\n",
       "      <td>0.394904</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>10617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139482</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.032000</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.048000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.825688</td>\n",
       "      <td>0.609091</td>\n",
       "      <td>0.054545</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>49567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102221</th>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.156463</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.115646</td>\n",
       "      <td>0.081633</td>\n",
       "      <td>0.299320</td>\n",
       "      <td>0.115646</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.129252</td>\n",
       "      <td>0.653061</td>\n",
       "      <td>0.319728</td>\n",
       "      <td>2015/4</td>\n",
       "      <td>35402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65879</th>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.031716</td>\n",
       "      <td>0.041045</td>\n",
       "      <td>0.145522</td>\n",
       "      <td>0.078358</td>\n",
       "      <td>0.022388</td>\n",
       "      <td>0.024254</td>\n",
       "      <td>0.095149</td>\n",
       "      <td>0.067164</td>\n",
       "      <td>0.011194</td>\n",
       "      <td>0.035448</td>\n",
       "      <td>0.177239</td>\n",
       "      <td>0.082090</td>\n",
       "      <td>2015/4</td>\n",
       "      <td>22419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135863</th>\n",
       "      <td>0.091139</td>\n",
       "      <td>0.126582</td>\n",
       "      <td>0.162025</td>\n",
       "      <td>0.048101</td>\n",
       "      <td>0.106329</td>\n",
       "      <td>0.189873</td>\n",
       "      <td>0.164557</td>\n",
       "      <td>0.111392</td>\n",
       "      <td>0.098734</td>\n",
       "      <td>0.116456</td>\n",
       "      <td>0.147959</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0.087629</td>\n",
       "      <td>2015/4</td>\n",
       "      <td>47934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135755</th>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.028777</td>\n",
       "      <td>0.050360</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.035971</td>\n",
       "      <td>0.043165</td>\n",
       "      <td>0.029197</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.021898</td>\n",
       "      <td>0.051095</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>2015/4</td>\n",
       "      <td>47798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170585</th>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.017857</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.029762</td>\n",
       "      <td>0.011905</td>\n",
       "      <td>2015/4</td>\n",
       "      <td>81579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>170586 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column1   Column2   Column3   Column4   Column5   Column6   Column7  \\\n",
       "0       0.317757  0.163551  1.000000  0.957447  1.000000  0.750000  0.979167   \n",
       "30078   0.164179  0.194030  0.138686  0.138686  0.109489  0.167883  0.160584   \n",
       "139428  0.042553  0.652482  0.836879  1.000000  0.295775  0.816901  0.295775   \n",
       "30024   0.203822  0.286624  0.159236  0.197452  0.178344  0.197452  0.140127   \n",
       "139482  0.000000  0.032000  0.016000  0.024000  0.032000  0.104000  0.352000   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "102221  0.047619  0.081633  0.156463  0.183673  0.115646  0.081633  0.299320   \n",
       "65879   0.119403  0.031716  0.041045  0.145522  0.078358  0.022388  0.024254   \n",
       "135863  0.091139  0.126582  0.162025  0.048101  0.106329  0.189873  0.164557   \n",
       "135755  0.028777  0.028777  0.050360  0.035971  0.035971  0.035971  0.043165   \n",
       "170585  0.029762  0.029762  0.029762  0.017857  0.041667  0.023810  0.017857   \n",
       "\n",
       "         Column8   Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
       "0       0.708333  0.979167  1.000000  0.641026  0.897436  1.000000   2002/3   \n",
       "30078   0.160584  0.160584  0.182482  0.153285  0.145985  0.167883   2002/3   \n",
       "139428  0.281690  0.492958  0.985915  0.436620  0.281690  0.535211   2002/3   \n",
       "30024   0.159236  0.210191  0.267516  0.248408  0.394904  1.000000   2002/3   \n",
       "139482  0.120000  0.048000  1.000000  0.825688  0.609091  0.054545   2002/3   \n",
       "...          ...       ...       ...       ...       ...       ...      ...   \n",
       "102221  0.115646  0.095238  0.129252  0.129252  0.653061  0.319728   2015/4   \n",
       "65879   0.095149  0.067164  0.011194  0.035448  0.177239  0.082090   2015/4   \n",
       "135863  0.111392  0.098734  0.116456  0.147959  0.107143  0.087629   2015/4   \n",
       "135755  0.029197  0.000000  0.007299  0.021898  0.051095  0.007299   2015/4   \n",
       "170585  0.023810  0.011905  0.023810  0.023810  0.029762  0.011905   2015/4   \n",
       "\n",
       "        Column15  \n",
       "0             23  \n",
       "30078      10755  \n",
       "139428     49541  \n",
       "30024      10617  \n",
       "139482     49567  \n",
       "...          ...  \n",
       "102221     35402  \n",
       "65879      22419  \n",
       "135863     47934  \n",
       "135755     47798  \n",
       "170585     81579  \n",
       "\n",
       "[170586 rows x 15 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Column1</th>\n",
       "      <th>Column2</th>\n",
       "      <th>Column3</th>\n",
       "      <th>Column4</th>\n",
       "      <th>Column5</th>\n",
       "      <th>Column6</th>\n",
       "      <th>Column7</th>\n",
       "      <th>Column8</th>\n",
       "      <th>Column9</th>\n",
       "      <th>Column10</th>\n",
       "      <th>Column11</th>\n",
       "      <th>Column12</th>\n",
       "      <th>Column13</th>\n",
       "      <th>Column14</th>\n",
       "      <th>Column15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24905</th>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.265306</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>0.367347</td>\n",
       "      <td>0.251701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.312925</td>\n",
       "      <td>0.292517</td>\n",
       "      <td>0.170068</td>\n",
       "      <td>0.209459</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.128378</td>\n",
       "      <td>2005/2</td>\n",
       "      <td>8751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155320</th>\n",
       "      <td>0.228972</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.049065</td>\n",
       "      <td>0.058411</td>\n",
       "      <td>0.144860</td>\n",
       "      <td>0.067757</td>\n",
       "      <td>0.058411</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.335404</td>\n",
       "      <td>0.198758</td>\n",
       "      <td>0.149068</td>\n",
       "      <td>0.186335</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>2006/3</td>\n",
       "      <td>56704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8745</th>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.114943</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>0.126437</td>\n",
       "      <td>0.159420</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>2015/2</td>\n",
       "      <td>3059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60462</th>\n",
       "      <td>0.307241</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>0.108527</td>\n",
       "      <td>0.300388</td>\n",
       "      <td>0.054264</td>\n",
       "      <td>0.031008</td>\n",
       "      <td>0.040698</td>\n",
       "      <td>0.253876</td>\n",
       "      <td>0.036320</td>\n",
       "      <td>0.019370</td>\n",
       "      <td>0.053269</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>2011/3</td>\n",
       "      <td>20496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4241</th>\n",
       "      <td>0.162963</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.177778</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.118519</td>\n",
       "      <td>0.140741</td>\n",
       "      <td>0.140741</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.103704</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.918519</td>\n",
       "      <td>0.096296</td>\n",
       "      <td>0.155556</td>\n",
       "      <td>2009/4</td>\n",
       "      <td>1242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58407</th>\n",
       "      <td>0.142132</td>\n",
       "      <td>0.157360</td>\n",
       "      <td>0.182741</td>\n",
       "      <td>0.096447</td>\n",
       "      <td>0.131980</td>\n",
       "      <td>0.147208</td>\n",
       "      <td>0.131980</td>\n",
       "      <td>0.131980</td>\n",
       "      <td>0.126904</td>\n",
       "      <td>0.147208</td>\n",
       "      <td>0.126904</td>\n",
       "      <td>0.096939</td>\n",
       "      <td>0.091837</td>\n",
       "      <td>2010/4</td>\n",
       "      <td>19892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124215</th>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.051282</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.128205</td>\n",
       "      <td>0.025641</td>\n",
       "      <td>0.012821</td>\n",
       "      <td>2006/2</td>\n",
       "      <td>43198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143532</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046154</td>\n",
       "      <td>0.015385</td>\n",
       "      <td>0.092308</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031250</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.046875</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>2002/3</td>\n",
       "      <td>51327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34430</th>\n",
       "      <td>0.080103</td>\n",
       "      <td>0.069767</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.095607</td>\n",
       "      <td>0.082687</td>\n",
       "      <td>0.058974</td>\n",
       "      <td>0.053846</td>\n",
       "      <td>0.105128</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.074359</td>\n",
       "      <td>0.056410</td>\n",
       "      <td>0.061538</td>\n",
       "      <td>2010/3</td>\n",
       "      <td>12261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75228</th>\n",
       "      <td>0.209877</td>\n",
       "      <td>0.152263</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.152091</td>\n",
       "      <td>0.190114</td>\n",
       "      <td>0.129278</td>\n",
       "      <td>0.045627</td>\n",
       "      <td>0.007605</td>\n",
       "      <td>0.019011</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.326996</td>\n",
       "      <td>0.075269</td>\n",
       "      <td>0.046595</td>\n",
       "      <td>2004/1</td>\n",
       "      <td>25336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160203</th>\n",
       "      <td>0.039451</td>\n",
       "      <td>0.079767</td>\n",
       "      <td>0.148936</td>\n",
       "      <td>0.075435</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.083012</td>\n",
       "      <td>0.074570</td>\n",
       "      <td>0.111922</td>\n",
       "      <td>0.166172</td>\n",
       "      <td>0.377622</td>\n",
       "      <td>0.657127</td>\n",
       "      <td>0.104895</td>\n",
       "      <td>0.391608</td>\n",
       "      <td>2012/2</td>\n",
       "      <td>58520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122866</th>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.291667</td>\n",
       "      <td>0.319444</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.180556</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.182692</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>2006/3</td>\n",
       "      <td>42882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95233</th>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.706897</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.137931</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>0.362069</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>2010/2</td>\n",
       "      <td>32999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47155</th>\n",
       "      <td>0.018476</td>\n",
       "      <td>0.041570</td>\n",
       "      <td>0.057737</td>\n",
       "      <td>0.055427</td>\n",
       "      <td>0.085450</td>\n",
       "      <td>0.048499</td>\n",
       "      <td>0.073903</td>\n",
       "      <td>0.030093</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.094907</td>\n",
       "      <td>0.095571</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2005/4</td>\n",
       "      <td>16423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76806</th>\n",
       "      <td>0.448980</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.163265</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.183673</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>2007/1</td>\n",
       "      <td>25919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17268</th>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.051724</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.034483</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.155172</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.068966</td>\n",
       "      <td>2013/1</td>\n",
       "      <td>6156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88503</th>\n",
       "      <td>0.012730</td>\n",
       "      <td>0.018388</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.089552</td>\n",
       "      <td>0.194030</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.262295</td>\n",
       "      <td>0.213115</td>\n",
       "      <td>0.295082</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.327869</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>2015/2</td>\n",
       "      <td>30192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75238</th>\n",
       "      <td>0.353909</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.053498</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.136882</td>\n",
       "      <td>0.068441</td>\n",
       "      <td>0.030418</td>\n",
       "      <td>0.068441</td>\n",
       "      <td>0.174905</td>\n",
       "      <td>0.049430</td>\n",
       "      <td>0.041825</td>\n",
       "      <td>0.050179</td>\n",
       "      <td>0.035842</td>\n",
       "      <td>2006/3</td>\n",
       "      <td>25336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114726</th>\n",
       "      <td>0.087248</td>\n",
       "      <td>0.070470</td>\n",
       "      <td>0.057047</td>\n",
       "      <td>0.036913</td>\n",
       "      <td>0.040268</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>0.033557</td>\n",
       "      <td>0.036913</td>\n",
       "      <td>0.057047</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.053691</td>\n",
       "      <td>0.046980</td>\n",
       "      <td>0.063758</td>\n",
       "      <td>2010/1</td>\n",
       "      <td>39988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129299</th>\n",
       "      <td>0.060764</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.022569</td>\n",
       "      <td>0.053819</td>\n",
       "      <td>0.027778</td>\n",
       "      <td>0.029514</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.015625</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>2008/2</td>\n",
       "      <td>45263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Column1   Column2   Column3   Column4   Column5   Column6   Column7  \\\n",
       "24905   0.222222  0.277778  0.265306  0.224490  0.367347  0.251701  1.000000   \n",
       "155320  0.228972  0.084112  0.049065  0.058411  0.144860  0.067757  0.058411   \n",
       "8745    0.114943  0.114943  0.103448  0.126437  0.126437  0.159420  0.150000   \n",
       "60462   0.307241  0.054264  0.032946  0.108527  0.300388  0.054264  0.031008   \n",
       "4241    0.162963  0.118519  0.177778  0.133333  0.118519  0.140741  0.140741   \n",
       "58407   0.142132  0.157360  0.182741  0.096447  0.131980  0.147208  0.131980   \n",
       "124215  0.038462  0.102564  0.166667  0.128205  0.128205  0.192308  0.179487   \n",
       "143532  0.000000  0.046154  0.015385  0.092308  0.061538  0.076923  0.000000   \n",
       "34430   0.080103  0.069767  0.056848  0.056848  0.095607  0.082687  0.058974   \n",
       "75228   0.209877  0.152263  0.259259  0.152091  0.190114  0.129278  0.045627   \n",
       "160203  0.039451  0.079767  0.148936  0.075435  0.071429  0.083012  0.074570   \n",
       "122866  0.375000  0.291667  0.319444  0.625000  0.111111  0.180556  0.125000   \n",
       "95233   0.034483  0.706897  1.000000  0.137931  0.155172  0.344828  0.068966   \n",
       "47155   0.018476  0.041570  0.057737  0.055427  0.085450  0.048499  0.073903   \n",
       "76806   0.448980  0.285714  0.163265  0.061224  0.163265  0.000000  0.183673   \n",
       "17268   0.051724  0.051724  0.068966  0.086207  0.034483  1.000000  0.155172   \n",
       "88503   0.012730  0.018388  0.089552  0.089552  0.194030  0.327869  0.262295   \n",
       "75238   0.353909  0.086420  0.053498  0.030418  0.136882  0.068441  0.030418   \n",
       "114726  0.087248  0.070470  0.057047  0.036913  0.040268  0.053691  0.033557   \n",
       "129299  0.060764  0.020833  0.022569  0.053819  0.027778  0.029514  0.015625   \n",
       "\n",
       "         Column8   Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
       "24905   0.312925  0.292517  0.170068  0.209459  0.216216  0.128378   2005/2   \n",
       "155320  0.260870  0.335404  0.198758  0.149068  0.186335  0.347826   2006/3   \n",
       "8745    0.150000  0.083333  0.116667  0.103448  0.155172  0.051724   2015/2   \n",
       "60462   0.040698  0.253876  0.036320  0.019370  0.053269  0.295400   2011/3   \n",
       "4241    0.074074  0.103704  0.111111  0.918519  0.096296  0.155556   2009/4   \n",
       "58407   0.131980  0.126904  0.147208  0.126904  0.096939  0.091837   2010/4   \n",
       "124215  0.076923  0.051282  0.064103  0.128205  0.025641  0.012821   2006/2   \n",
       "143532  0.031250  0.062500  0.000000  0.000000  0.046875  0.062500   2002/3   \n",
       "34430   0.053846  0.105128  0.133333  0.074359  0.056410  0.061538   2010/3   \n",
       "75228   0.007605  0.019011  0.030418  0.326996  0.075269  0.046595   2004/1   \n",
       "160203  0.111922  0.166172  0.377622  0.657127  0.104895  0.391608   2012/2   \n",
       "122866  0.028846  0.182692  0.192308  0.028846  0.038462  0.153846   2006/3   \n",
       "95233   0.034483  0.362069  0.068966  0.120690  0.155172  0.103448   2010/2   \n",
       "47155   0.030093  0.074074  0.104167  0.094907  0.095571  1.000000   2005/4   \n",
       "76806   0.375000  0.416667  0.708333  0.437500  0.145833  0.083333   2007/1   \n",
       "17268   0.103448  0.103448  0.120690  0.086207  0.103448  0.068966   2013/1   \n",
       "88503   0.213115  0.295082  0.344262  0.327869  0.557377  0.377049   2015/2   \n",
       "75238   0.068441  0.174905  0.049430  0.041825  0.050179  0.035842   2006/3   \n",
       "114726  0.036913  0.057047  0.046980  0.053691  0.046980  0.063758   2010/1   \n",
       "129299  0.020833  0.020833  0.015625  0.200000  0.260000  0.220000   2008/2   \n",
       "\n",
       "        Column15  \n",
       "24905       8751  \n",
       "155320     56704  \n",
       "8745        3059  \n",
       "60462      20496  \n",
       "4241        1242  \n",
       "58407      19892  \n",
       "124215     43198  \n",
       "143532     51327  \n",
       "34430      12261  \n",
       "75228      25336  \n",
       "160203     58520  \n",
       "122866     42882  \n",
       "95233      32999  \n",
       "47155      16423  \n",
       "76806      25919  \n",
       "17268       6156  \n",
       "88503      30192  \n",
       "75238      25336  \n",
       "114726     39988  \n",
       "129299     45263  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el modelo preentrenado\n",
    "model = load_model('modelTrainedv2.h5', custom_objects={'custom_mre': custom_mre})\n",
    "\n",
    "# Definir una función para construir el modelo con un learning rate ajustable\n",
    "def build_model(hp):\n",
    "    # Crear una nueva capa de entrada con 12 neuronas\n",
    "    nueva_capa_entrada = Input(shape=(12,))\n",
    "    \n",
    "    # Construir un nuevo modelo que incluya la nueva capa de entrada\n",
    "    new_model = Sequential()\n",
    "    new_model.add(nueva_capa_entrada)\n",
    "\n",
    "    # Redimensionar la capa dense_1 para que sea compatible con 12 neuronas de entrada\n",
    "    new_model.add(Dense(30, activation='tanh', input_shape=(12,), name=\"Input\"))\n",
    "\n",
    "    # Congelar las primeras 12 capas\n",
    "    numFreezeLayers = 6\n",
    "\n",
    "    # Agregar las capas restantes del modelo preentrenado (excluyendo la capa de entrada original)\n",
    "    for layer in model.layers[1:numFreezeLayers]:\n",
    "        new_model.add(layer)\n",
    "   \n",
    "    for capa in new_model.layers[0:numFreezeLayers+1]:\n",
    "        capa.trainable = False\n",
    "\n",
    "    # Agregar las capas del modelo preentrenado (excluyendo la capa de entrada original)\n",
    "    for i, layer in enumerate(model.layers[numFreezeLayers:]):  # Excluir la capa de entrada\n",
    "        # Solo modificar capas entrenables (trainable=True)\n",
    "        if isinstance(layer, Dense):  # Solo modificar capas Dense\n",
    "            if layer.trainable:\n",
    "                # Ajustamos el número de neuronas solo en las capas entrenables\n",
    "                dense_units = hp.Int(f'HL_{numFreezeLayers+i}_units', min_value=20, max_value=100, step=10)  # Neuronas ajustables\n",
    "                new_model.add(Dense(dense_units, activation=layer.activation, name=f\"dense_tunable_{numFreezeLayers+i}\"))\n",
    "            else:\n",
    "                # Si la capa no es entrenable, agregamos la capa tal cual está\n",
    "                new_model.add(layer)\n",
    "        else:\n",
    "            # Si no es una capa Dense (por ejemplo, Dropout, Flatten, etc.), la agregamos tal cual\n",
    "            new_model.add(layer)\n",
    "\n",
    "    # Agregar una nueva capa de salida con 1 neurona\n",
    "    new_model.add(Dense(1, activation='tanh', name='Output'))\n",
    "\n",
    "    # Definir el optimizador con un learning rate ajustable\n",
    "    optimizer = Adam(\n",
    "        learning_rate=hp.Float('learning_rate', min_value=0.00001, max_value=0.001, sampling='log')\n",
    "    )\n",
    "\n",
    "    # Compilar el modelo\n",
    "    new_model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='mean_absolute_error',\n",
    "        metrics=['mean_absolute_error','mean_squared_error']\n",
    "    )\n",
    "    # Obtener las funciones de activación asignadas\n",
    "    activations = [layer.activation.__name__ for layer in new_model.layers]\n",
    "    print(activations)\n",
    "\n",
    "    # Obtener funciones de activación y entrenabilidad asignadas\n",
    "    layer_info = [layer.trainable for layer in new_model.layers]\n",
    "    print(layer_info)\n",
    "\n",
    "    new_model.summary()\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 20 Complete [00h 02m 32s]\n",
      "val_mean_absolute_error: 0.08535069972276688\n",
      "\n",
      "Best val_mean_absolute_error So Far: 0.08535069972276688\n",
      "Total elapsed time: 01h 00m 22s\n",
      "Tiempo de optimizacion (HH:MM:SS): 1:0:22\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "# Configurar el tuner para la optimización bayesiana\n",
    "tuner = kt.BayesianOptimization(\n",
    "    build_model,\n",
    "    objective=kt.Objective(\"val_mean_absolute_error\",direction=\"min\"),\n",
    "    #objective='val_loss',\n",
    "    max_trials=20,  # Número máximo de configuraciones a probar\n",
    "    directory='bayesian_optimization',\n",
    "    project_name='transfer_learning_elec-water_6freeze',\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Ejecutar la búsqueda de hiperparámetros\n",
    "tuner.search(\n",
    "    X_train_norm, y_train_norm,\n",
    "    validation_data=(X_val_norm, y_val_norm),\n",
    "    epochs=100,  # Número máximo de épocas\n",
    "    batch_size=256,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        min_delta=1E-4,\n",
    "        restore_best_weights=True\n",
    "    )]\n",
    ")\n",
    "end_time = time.time()\n",
    "\n",
    "optimization_time = end_time - start_time\n",
    "# Imprime el tiempo de entrenamiento en segundos y en formato de horas, minutos y segundos\n",
    "print(f'Tiempo de optimizacion (HH:MM:SS): {int(optimization_time // 3600)}:{int((optimization_time % 3600) // 60)}:{int(optimization_time % 60)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor tasa de aprendizaje encontrada: 0.0007282286591355298\n",
      "['tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh', 'tanh']\n",
      "[False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 80)                2480      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 90)                7290      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 60)                5460      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 60)                3660      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               6100      \n",
      "                                                                 \n",
      " dense_tunable_6 (Dense)     (None, 40)                4040      \n",
      "                                                                 \n",
      " dense_tunable_7 (Dense)     (None, 50)                2050      \n",
      "                                                                 \n",
      " dense_tunable_8 (Dense)     (None, 70)                3570      \n",
      "                                                                 \n",
      " dense_tunable_9 (Dense)     (None, 50)                3550      \n",
      "                                                                 \n",
      " dense_tunable_10 (Dense)    (None, 60)                3060      \n",
      "                                                                 \n",
      " dense_tunable_11 (Dense)    (None, 30)                1830      \n",
      "                                                                 \n",
      " dense_tunable_12 (Dense)    (None, 100)               3100      \n",
      "                                                                 \n",
      " dense_tunable_13 (Dense)    (None, 90)                9090      \n",
      "                                                                 \n",
      " dense_tunable_14 (Dense)    (None, 20)                1820      \n",
      "                                                                 \n",
      " dense_tunable_15 (Dense)    (None, 80)                1680      \n",
      "                                                                 \n",
      " dense_tunable_16 (Dense)    (None, 100)               8100      \n",
      "                                                                 \n",
      " dense_tunable_17 (Dense)    (None, 70)                7070      \n",
      "                                                                 \n",
      " dense_tunable_18 (Dense)    (None, 100)               7100      \n",
      "                                                                 \n",
      " dense_tunable_19 (Dense)    (None, 60)                6060      \n",
      "                                                                 \n",
      " dense_tunable_20 (Dense)    (None, 70)                4270      \n",
      "                                                                 \n",
      " dense_tunable_21 (Dense)    (None, 60)                4260      \n",
      "                                                                 \n",
      " dense_tunable_22 (Dense)    (None, 100)               6100      \n",
      "                                                                 \n",
      " dense_tunable_23 (Dense)    (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_tunable_24 (Dense)    (None, 20)                1620      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 111851 (436.92 KB)\n",
      "Trainable params: 86471 (337.78 KB)\n",
      "Non-trainable params: 25380 (99.14 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/200\n",
      "327/327 [==============================] - 6s 10ms/step - loss: 0.0856 - mean_absolute_error: 0.0856 - mean_squared_error: 0.0261 - val_loss: 0.0859 - val_mean_absolute_error: 0.0859 - val_mean_squared_error: 0.0261\n",
      "Epoch 2/200\n",
      "327/327 [==============================] - 3s 8ms/step - loss: 0.0859 - mean_absolute_error: 0.0859 - mean_squared_error: 0.0261 - val_loss: 0.0885 - val_mean_absolute_error: 0.0885 - val_mean_squared_error: 0.0266\n",
      "Epoch 3/200\n",
      "327/327 [==============================] - 3s 8ms/step - loss: 0.0858 - mean_absolute_error: 0.0858 - mean_squared_error: 0.0261 - val_loss: 0.0852 - val_mean_absolute_error: 0.0852 - val_mean_squared_error: 0.0263\n",
      "Epoch 4/200\n",
      "327/327 [==============================] - 3s 8ms/step - loss: 0.0857 - mean_absolute_error: 0.0857 - mean_squared_error: 0.0261 - val_loss: 0.0862 - val_mean_absolute_error: 0.0862 - val_mean_squared_error: 0.0262\n",
      "Epoch 5/200\n",
      "327/327 [==============================] - 3s 8ms/step - loss: 0.0856 - mean_absolute_error: 0.0856 - mean_squared_error: 0.0261 - val_loss: 0.0861 - val_mean_absolute_error: 0.0861 - val_mean_squared_error: 0.0270\n",
      "Epoch 6/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0856 - mean_absolute_error: 0.0856 - mean_squared_error: 0.0261 - val_loss: 0.0882 - val_mean_absolute_error: 0.0882 - val_mean_squared_error: 0.0257\n",
      "Epoch 7/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - mean_squared_error: 0.0260 - val_loss: 0.0859 - val_mean_absolute_error: 0.0859 - val_mean_squared_error: 0.0266\n",
      "Epoch 8/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0855 - mean_absolute_error: 0.0855 - mean_squared_error: 0.0261 - val_loss: 0.0865 - val_mean_absolute_error: 0.0865 - val_mean_squared_error: 0.0261\n",
      "Epoch 9/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - mean_squared_error: 0.0260 - val_loss: 0.0863 - val_mean_absolute_error: 0.0863 - val_mean_squared_error: 0.0264\n",
      "Epoch 10/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0853 - mean_absolute_error: 0.0853 - mean_squared_error: 0.0259 - val_loss: 0.0856 - val_mean_absolute_error: 0.0856 - val_mean_squared_error: 0.0267\n",
      "Epoch 11/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0853 - mean_absolute_error: 0.0853 - mean_squared_error: 0.0259 - val_loss: 0.0872 - val_mean_absolute_error: 0.0872 - val_mean_squared_error: 0.0268\n",
      "Epoch 12/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - mean_squared_error: 0.0259 - val_loss: 0.0853 - val_mean_absolute_error: 0.0853 - val_mean_squared_error: 0.0265\n",
      "Epoch 13/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0854 - mean_absolute_error: 0.0854 - mean_squared_error: 0.0260 - val_loss: 0.0860 - val_mean_absolute_error: 0.0860 - val_mean_squared_error: 0.0269\n",
      "Epoch 14/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0852 - mean_absolute_error: 0.0852 - mean_squared_error: 0.0259 - val_loss: 0.0857 - val_mean_absolute_error: 0.0857 - val_mean_squared_error: 0.0269\n",
      "Epoch 15/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0852 - mean_absolute_error: 0.0852 - mean_squared_error: 0.0259 - val_loss: 0.0861 - val_mean_absolute_error: 0.0861 - val_mean_squared_error: 0.0274\n",
      "Epoch 16/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0850 - mean_absolute_error: 0.0850 - mean_squared_error: 0.0259 - val_loss: 0.0863 - val_mean_absolute_error: 0.0863 - val_mean_squared_error: 0.0266\n",
      "Epoch 17/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0849 - mean_absolute_error: 0.0849 - mean_squared_error: 0.0258 - val_loss: 0.0859 - val_mean_absolute_error: 0.0859 - val_mean_squared_error: 0.0268\n",
      "Epoch 18/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0849 - mean_absolute_error: 0.0849 - mean_squared_error: 0.0258 - val_loss: 0.0861 - val_mean_absolute_error: 0.0861 - val_mean_squared_error: 0.0263\n",
      "Epoch 19/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0849 - mean_absolute_error: 0.0849 - mean_squared_error: 0.0258 - val_loss: 0.0855 - val_mean_absolute_error: 0.0855 - val_mean_squared_error: 0.0265\n",
      "Epoch 20/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0847 - mean_absolute_error: 0.0847 - mean_squared_error: 0.0258 - val_loss: 0.0861 - val_mean_absolute_error: 0.0861 - val_mean_squared_error: 0.0268\n",
      "Epoch 21/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0847 - mean_absolute_error: 0.0847 - mean_squared_error: 0.0257 - val_loss: 0.0863 - val_mean_absolute_error: 0.0863 - val_mean_squared_error: 0.0259\n",
      "Epoch 22/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0848 - mean_absolute_error: 0.0848 - mean_squared_error: 0.0258 - val_loss: 0.0856 - val_mean_absolute_error: 0.0856 - val_mean_squared_error: 0.0261\n",
      "Epoch 23/200\n",
      "327/327 [==============================] - 2s 7ms/step - loss: 0.0849 - mean_absolute_error: 0.0849 - mean_squared_error: 0.0257 - val_loss: 0.0865 - val_mean_absolute_error: 0.0865 - val_mean_squared_error: 0.0272\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Input (Dense)               (None, 30)                390       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 80)                2480      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 90)                7290      \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 60)                5460      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 60)                3660      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               6100      \n",
      "                                                                 \n",
      " dense_tunable_6 (Dense)     (None, 40)                4040      \n",
      "                                                                 \n",
      " dense_tunable_7 (Dense)     (None, 50)                2050      \n",
      "                                                                 \n",
      " dense_tunable_8 (Dense)     (None, 70)                3570      \n",
      "                                                                 \n",
      " dense_tunable_9 (Dense)     (None, 50)                3550      \n",
      "                                                                 \n",
      " dense_tunable_10 (Dense)    (None, 60)                3060      \n",
      "                                                                 \n",
      " dense_tunable_11 (Dense)    (None, 30)                1830      \n",
      "                                                                 \n",
      " dense_tunable_12 (Dense)    (None, 100)               3100      \n",
      "                                                                 \n",
      " dense_tunable_13 (Dense)    (None, 90)                9090      \n",
      "                                                                 \n",
      " dense_tunable_14 (Dense)    (None, 20)                1820      \n",
      "                                                                 \n",
      " dense_tunable_15 (Dense)    (None, 80)                1680      \n",
      "                                                                 \n",
      " dense_tunable_16 (Dense)    (None, 100)               8100      \n",
      "                                                                 \n",
      " dense_tunable_17 (Dense)    (None, 70)                7070      \n",
      "                                                                 \n",
      " dense_tunable_18 (Dense)    (None, 100)               7100      \n",
      "                                                                 \n",
      " dense_tunable_19 (Dense)    (None, 60)                6060      \n",
      "                                                                 \n",
      " dense_tunable_20 (Dense)    (None, 70)                4270      \n",
      "                                                                 \n",
      " dense_tunable_21 (Dense)    (None, 60)                4260      \n",
      "                                                                 \n",
      " dense_tunable_22 (Dense)    (None, 100)               6100      \n",
      "                                                                 \n",
      " dense_tunable_23 (Dense)    (None, 80)                8080      \n",
      "                                                                 \n",
      " dense_tunable_24 (Dense)    (None, 20)                1620      \n",
      "                                                                 \n",
      " Output (Dense)              (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 111851 (436.92 KB)\n",
      "Trainable params: 86471 (337.78 KB)\n",
      "Non-trainable params: 25380 (99.14 KB)\n",
      "_________________________________________________________________\n",
      "Tiempo de entrenamiento: 58.24 segundos\n"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo con Early Stopping\n",
    "num_epochs = 200\n",
    "batch_size = 256\n",
    "\n",
    "# Obtener el mejor learning rate\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "best_lr = best_hps.get('learning_rate')\n",
    "print(f\"Mejor tasa de aprendizaje encontrada: {best_lr}\")\n",
    "\n",
    "# Comienza a medir el tiempo de entrenamiento\n",
    "start_time = time.time()\n",
    "# Entrenar el mejor modelo con el learning rate óptimo\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "history = best_model.fit(\n",
    "    X_train_norm, y_train_norm,\n",
    "    validation_data=(X_val_norm, y_val_norm),\n",
    "    epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=20,\n",
    "        min_delta=1E-4,\n",
    "        restore_best_weights=True\n",
    "    )]\n",
    ")\n",
    "# Finaliza la medición del tiempo de entrenamiento\n",
    "end_time = time.time()\n",
    "\n",
    "# Resumen del modelo final\n",
    "best_model.summary()\n",
    "\n",
    "training_time = end_time - start_time\n",
    "# Imprime el tiempo de entrenamiento en segundos y en formato de horas, minutos y segundos\n",
    "print(f'Tiempo de entrenamiento: {training_time:.2f} segundos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tiempo total de entrenamiento: 58.24416399002075 segundos\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraer la información de la función objetivo desde el historial de entrenamiento\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "# Crear un gráfico con los valores de la función objetivo respecto a las épocas\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, len(train_loss) + 1), train_loss, label='Training Loss')\n",
    "plt.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "\n",
    "# Ajustar los límites de los ejes\n",
    "plt.ylim(0.080, 0.115)\n",
    "plt.xlim(0, 100)\n",
    "\n",
    "# Agregar etiquetas y leyenda al gráfico\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Mostrar el gráfico\n",
    "plt.show()\n",
    "\n",
    "# Imprimir el tiempo total de entrenamiento\n",
    "print(f\"Tiempo total de entrenamiento: {end_time - start_time} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 3s 2ms/step\n",
      "[[0.21475844]\n",
      " [0.30406347]\n",
      " [0.11228094]\n",
      " ...\n",
      " [0.08054744]\n",
      " [0.33208784]\n",
      " [0.45879152]]\n",
      "       Valor Real  Predicciones\n",
      "0        0.128378      0.214758\n",
      "1        0.347826      0.304063\n",
      "2        0.051724      0.112281\n",
      "3        0.295400      0.242329\n",
      "4        0.155556      0.078351\n",
      "...           ...           ...\n",
      "51171    0.321429      0.140425\n",
      "51172    0.011936      0.034552\n",
      "51173    0.053030      0.080547\n",
      "51174    0.312155      0.332088\n",
      "51175    0.600000      0.458792\n",
      "\n",
      "[51176 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AdrianGilGamboa\\AppData\\Local\\Temp\\ipykernel_10964\\3725175599.py:30: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[19.85624993 20.13032909 23.20633167 19.17840584 18.27235705 17.09780576\n",
      " 19.60733233 18.77521829 38.68144959 18.44887818 45.26714838 20.01876372\n",
      " 44.32860804 16.72862334 17.85519614 19.63205163 22.08992426]' has dtype incompatible with float32, please explicitly cast to a compatible dtype first.\n",
      "  desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Column1  Column2  Column3  Column4  Column5  Column6  Column7  \\\n",
      "24905      58.0     66.0     62.0     56.0     77.0     60.0    170.0   \n",
      "155320    101.0     39.0     24.0     28.0     65.0     32.0     28.0   \n",
      "8745       30.0     30.0     29.0     31.0     31.0     31.0     29.0   \n",
      "60462     193.0     59.0     48.0     87.0    186.0     59.0     47.0   \n",
      "4241       35.0     29.0     37.0     31.0     29.0     32.0     32.0   \n",
      "...         ...      ...      ...      ...      ...      ...      ...   \n",
      "132964     37.0     35.0     33.0     35.0     34.0     36.0     21.0   \n",
      "130033      5.0      8.0      7.0     11.0      1.0      7.0      8.0   \n",
      "124375    137.0    120.0     63.0     60.0     54.0     73.0      6.0   \n",
      "50855     185.0     32.0     49.0     67.0     51.0     37.0     56.0   \n",
      "5442       29.0     65.0     44.0     31.0     50.0     69.0     48.0   \n",
      "\n",
      "        Column8  Column9  Column10  Column11  Column12  Column13 Column14  \\\n",
      "24905      69.0     66.0      48.0      53.0  0.216216      41.0   2005/2   \n",
      "155320     45.0     57.0      35.0      27.0  0.186335      59.0   2006/3   \n",
      "8745       29.0     25.0      27.0      26.0  0.155172      23.0   2015/2   \n",
      "60462      52.0    162.0      43.0      36.0  0.053269     150.0   2011/3   \n",
      "4241       23.0     27.0      28.0     137.0  0.096296      34.0   2009/4   \n",
      "...         ...      ...       ...       ...       ...       ...      ...   \n",
      "132964     22.0     44.0      31.0      26.0  0.090909      65.5   2006/3   \n",
      "130033      6.0     12.0       8.0      11.0  0.007958      10.0   2002/4   \n",
      "124375     26.0     32.0      14.0      14.0  0.060606      13.0   2005/4   \n",
      "50855     159.0    109.0      56.0      70.0  0.908840     145.0   2012/4   \n",
      "5442       40.0     55.0      53.0      52.0  0.463636      85.0   2013/1   \n",
      "\n",
      "        Column15  Predicted_Column13  \n",
      "24905       8751           53.784249  \n",
      "155320     56704           51.954219  \n",
      "8745        3059           26.512295  \n",
      "60462      20496          128.082056  \n",
      "4241        1242           23.577444  \n",
      "...          ...                 ...  \n",
      "132964     46569           19.931903  \n",
      "130033     45459           27.052220  \n",
      "124375     43274           16.632262  \n",
      "50855      17695          152.215800  \n",
      "5442        1656           69.467068  \n",
      "\n",
      "[51176 rows x 16 columns]\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           53.784249\n",
      "155320      59.0   2006/3     56704           51.954219\n",
      "8745        23.0   2015/2      3059           26.512295\n",
      "60462      150.0   2011/3     20496          128.082056\n",
      "4241        34.0   2009/4      1242           23.577444\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           19.931903\n",
      "130033      10.0   2002/4     45459           27.052220\n",
      "124375      13.0   2005/4     43274           16.632262\n",
      "50855      145.0   2012/4     17695          152.215800\n",
      "5442        85.0   2013/1      1656           69.467068\n",
      "\n",
      "[51176 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = best_model.predict(X_test_norm)\n",
    "print(y_pred)\n",
    "\n",
    "# Crear un DataFrame con las predicciones desnormalizadas y los valores reales\n",
    "resultados = pd.DataFrame({'Valor Real': y_test_norm.values.flatten(), 'Predicciones': y_pred.flatten()})\n",
    "print(resultados)\n",
    "\n",
    "# Agregar la columna de predicciones al conjunto de prueba\n",
    "test_data['Predicted_Column13'] = y_pred.flatten()\n",
    "\n",
    "# Crear un DataFrame para almacenar los resultados desnormalizados\n",
    "desnormalized_test_data = test_data.copy()\n",
    "\n",
    "# Desnormalizar 'Column1' a 'Column13' y 'Predicted_Column13' según la normalización por grupos\n",
    "for group, scalerY in scalers.items():\n",
    "    # Filtrar el conjunto de prueba correspondiente al grupo\n",
    "    group_test_data = test_data[test_data['Column15'] == group]\n",
    "\n",
    "    # Seleccionar las columnas normalizadas para desnormalizar\n",
    "    normalized_features = group_test_data[['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']]\n",
    "\n",
    "    # Desnormalizar los datos utilizando el objeto scalerY correspondiente\n",
    "    original_data = scalerY.inverse_transform(normalized_features)\n",
    "\n",
    "    # Crear un DataFrame temporal para almacenar los datos desnormalizados\n",
    "    temp_df = pd.DataFrame(original_data, columns=['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13'])\n",
    "\n",
    "    # Actualizar el DataFrame desnormalizado con los datos desnormalizados\n",
    "    desnormalized_test_data.loc[desnormalized_test_data['Column15'] == group, ['Column1', 'Column2', 'Column3', 'Column4', 'Column5', 'Column6', 'Column7', 'Column8', 'Column9', 'Column10', 'Column11', 'Column13', 'Predicted_Column13']] = temp_df.values\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la desnormalización\n",
    "print(desnormalized_test_data)\n",
    "\n",
    "# Eliminar todas las columnas excepto las últimas cuatro\n",
    "resultados = desnormalized_test_data.iloc[:, -4:]\n",
    "\n",
    "# Imprimir el conjunto de prueba después de la eliminación de columnas\n",
    "print(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test_norm: 24905      41.0\n",
      "155320     59.0\n",
      "8745       23.0\n",
      "60462     150.0\n",
      "4241       34.0\n",
      "          ...  \n",
      "132964     65.5\n",
      "130033     10.0\n",
      "124375     13.0\n",
      "50855     145.0\n",
      "5442       85.0\n",
      "Name: Column13, Length: 51176, dtype: float64\n",
      "y_pred: 24905      53.784249\n",
      "155320     51.954219\n",
      "8745       26.512295\n",
      "60462     128.082056\n",
      "4241       23.577444\n",
      "             ...    \n",
      "132964     19.931903\n",
      "130033     27.052220\n",
      "124375     16.632262\n",
      "50855     152.215800\n",
      "5442       69.467068\n",
      "Name: Predicted_Column13, Length: 51176, dtype: float64\n",
      "        Column13 Column14  Column15  Predicted_Column13\n",
      "24905       41.0   2005/2      8751           53.784249\n",
      "155320      59.0   2006/3     56704           51.954219\n",
      "8745        23.0   2015/2      3059           26.512295\n",
      "60462      150.0   2011/3     20496          128.082056\n",
      "4241        34.0   2009/4      1242           23.577444\n",
      "...          ...      ...       ...                 ...\n",
      "132964      65.5   2006/3     46569           19.931903\n",
      "130033      10.0   2002/4     45459           27.052220\n",
      "124375      13.0   2005/4     43274           16.632262\n",
      "50855      145.0   2012/4     17695          152.215800\n",
      "5442        85.0   2013/1      1656           69.467068\n",
      "\n",
      "[51176 rows x 4 columns]\n",
      "RMSE en el conjunto de prueba: 43.23097950821096\n",
      "MAE en el conjunto de prueba: 13.059740054782395\n",
      "MAPE en el conjunto de prueba: 48.25379250884346\n",
      "Tiempo de entrenamiento: 58.24 segundos\n"
     ]
    }
   ],
   "source": [
    "# Obtener y_test_norm de la primera columna de resultados\n",
    "y_test_norm = resultados['Column13']\n",
    "\n",
    "# Obtener y_pred de la última columna del conjunto de prueba después de la desnormalización\n",
    "y_pred = desnormalized_test_data['Predicted_Column13']\n",
    "\n",
    "# Imprimir y_test_norm y y_pred\n",
    "print(\"y_test_norm:\", y_test_norm)\n",
    "print(\"y_pred:\", y_pred)\n",
    "\n",
    "\n",
    "# Imprimir el DataFrame\n",
    "print(resultados)\n",
    "# Guardar el DataFrame resultados en un archivo CSV\n",
    "resultados.to_csv('resultadosBayesianOpt6Freeze.csv', index=False)\n",
    "\n",
    "# Calcular RMSE con datos desnormalizados\n",
    "rmse = np.sqrt(mean_squared_error(y_test_norm, y_pred))\n",
    "print(f'RMSE en el conjunto de prueba: {rmse}')\n",
    "\n",
    "# Calcular MAE con datos desnormalizados\n",
    "mae = mean_absolute_error(y_test_norm, y_pred)\n",
    "print(f'MAE en el conjunto de prueba: {mae}')\n",
    "\n",
    "# Calcular MRE con datos desnormalizados\n",
    "mre = custom_mre(y_test_norm, y_pred)\n",
    "print(f'MAPE en el conjunto de prueba: {mre}')\n",
    "\n",
    "#Calcular MAPE con datos desnormalizados\n",
    "#print(\"Mean absolute percentage error (MAPE): %f\" % mean_absolute_percentage_error(y_test_norm, y_pred))\n",
    "\n",
    "# Calcula la duración del entrenamiento en segundos\n",
    "training_duration = end_time - start_time\n",
    "# Imprime el tiempo de entrenamiento en segundos y en formato de horas, minutos y segundos\n",
    "print(f'Tiempo de entrenamiento: {training_duration:.2f} segundos')\n",
    "#print(f'Tiempo de entrenamiento (HH:MM:SS): {int(training_duration // 3600)}:{int((training_duration % 3600) // 60)}:{int(training_duration % 60)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.08 MAPE por año"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
